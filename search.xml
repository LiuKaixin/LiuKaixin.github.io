<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据集熟悉]]></title>
    <url>%2F2017%2F11%2F28%2F%E6%95%B0%E6%8D%AE%E9%9B%86%E7%86%9F%E6%82%89%2F</url>
    <content type="text"><![CDATA[目前dbpedia和YAGO提供的可视化工具其实都比较弱，需要一个一个的手动添加点，进行可视化。不过他们都是基于wiki百科，如果要想例子，估计只能拍脑门想概念了，所以就先这个工作先放一放吧，读一读师兄说的构建索引的文章看看。 在阅读《Fast Hierarchy Construction for Dense Subgraphs》时，看到了数据集稠密这样的字眼。想到还是应该对数据集整体有个把握，调查到两个可视化工具VOSViewer和NWB Tool，还有相应的调研论文，知乎再次对Dbpedia进行调研~ DBpedia-2016-10： Statistics The English version of the DBpedia knowledge base currently describes 6.6M entities of which 4.9M have abstracts, 1.9M have geo coordinates and 1.7M depictions. In total, 5.5M resources are classified in a consistent ontology, consisting of 1.5M persons, 840K places (including 513K populated places), 496K works (including 139K music albums, 111K films and 21K video games), 286K organizations (including 70K companies and 55K educational institutions), 306K species, 58K plants and 6K diseases. The total number of resources in English DBpedia is 18M that, besides the 6.6M resources, includes 1.7M skos concepts (categories), 7.7M redirect pages, 269K disambiguation pages and 1.7M intermediate nodes. Altogether the DBpedia 2016-10 release consists of 13 billion (2016-04: 11.5 billion) pieces of information (RDF triples) out of which 1.7 billion (2016-04: 1.6 billion) were extracted from the English edition of Wikipedia, 6.6 billion (2016-04: 6 billion) were extracted from other language editions and 4.8 billion (2016-04: 4 billion) from Wikipedia Commons and Wikidata. In addition, adding the large NIF datasets for each language edition (see details below) increased the number of triples further by over 9 billion, bringing the overall count up to 23 billion triples.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Fast Hierarchy Construction for Dense Subgraphs]]></title>
    <url>%2F2017%2F11%2F28%2FFast-Hierarchy-Construction-for-Dense-Subgraphs%2F</url>
    <content type="text"><![CDATA[学习建立索引的方法。2017/11/30 哇给跪了。。。第二章的定义都是什么鬼？！2017/12/15 放弃了 ABSTRACT意义：在稠密子图上建立hierarchy，Peeling算法（k-core,k-truss, nucleus decomposition）对找到稠密子图很有效，但稠密结构的分层表示和k-core, k-truss的正确计算，都被忽略了。工作：对不同问题定义进行调研，然后提出有效和通用的算法构造k-core, k-truss或nucleus decomposition的层次结构。Our algorithms leverage the disjoint-set forest data structure to efficiently construct the hierarchy during traversal. 为避免遍历，在peeling访问邻居时，构建子图，并考虑与之前子图的关系。 1. INTRODUCTION真实世界的图是稀疏的，但节点的邻居是稠密的，其聚类系数和传递性也很高。在各种应用中关于稠密子图发现有着众多的文献。举例社交网络，股票市场，DNA等。peeling algorithms —— k-core, k-truss, nucleus decomposition.Hierarchy是复杂网络的中心组织原则，对于将图的communities联系起来和洞察一些图的现象十分有用。peeling algorithms 可支持稠密图的Hierarchy发现。 1.1 Problem, Misconception and ChallengesProblem: undirected unattributed graphs. tree — hierarchy of dense subgraphs. node — subgraph, edge — containment relation, root — entire graph. aim — find hierarchy using peeling algorithms. Misconception in the literature:Recent studies on peeling algorithms has interestingly overlooked the connectivity condition of k-cores and k-trusses. Challenges:cost of traversals and hierarchy construction.k-core, k-truss, nucleus decompositions需要在遍历图之后进行。层次结构需要在得到k-core, k-truss, nucleus decomposition之后。但在整个图上一次遍历时，对嵌套结构进行遍历并不容易。 1.2 Contributions Thorough literature review.指出现有文献在k-core和k-truss定义上的误解，并指出缺少hierarchy的理解，它的成本和peeling过程相当。 Hierarchy construction by disjoint-set forest: 使用不相交的森林数据结构跟踪出现在层次结构树中相同节点中不相交的子结构。通过特定的顺序处理子图，将不相交集的森林结合到层次树中。 Avoiding traversal: 构建层次结构时不用遍历。在peeling过程中，在访问邻居时构建子图，并记录和之前子图的关系。对记录的关系应用轻量的post-processing得到hierarchy。 Experimental evaluation: 2. PERLIMINARIES2.1 Nucleus decomposition G——an undirected and simple graph.$K_r$——an r-clique. 这个定义感觉挺厉害的，是一般情况下(r=1，s=2)节点的度和连通性的generalization。 举例：若r=2，s=3：则每个边至少在k个三角形中，而且每个边被三角形联通。 For an r-clique $K_r$ u, $w_s(u)$ denotes the $K_s$-degree of u. For a subgraph $H\subset G, w_{r,s}(H)$ is defined as the minimum $K_s$-degreee of a $K_r$ in H, i.e., $w_{r,s}(H)= min\{w_s(u): u\in H\}$.]]></content>
  </entry>
  <entry>
    <title><![CDATA[《Summarizing Answer Graphs Induced by Keyword Queries》——论文笔记]]></title>
    <url>%2F2017%2F11%2F06%2F%E3%80%8ASummarizing-Answer-Graphs-Induced-by-Keyword-Queries%E3%80%8B%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[小结：指出关键词和结构化查询各自的缺点。然后对关键词查出的答案进行摘要总结，将同一类型的答案归为一个摘要图。返回给用户使其得到graph queries。和ICDE09的文章有点儿像。其指出和ICDE09相比不需要schema。（我觉得这是在耍赖。。他对搜好的答案进行摘要感觉结果小多了肯定不需要schema。。。）在得到答案后ICDE09，是将其转为SPARQL，供用户选择本文则是直接进行摘要，让用户进行选择，然后自己构建SPARQL（虽然没仔细看算法，但这个VLDB应该厉害在自己定义了个新的问题，并且对这个问题进行了较为全面的考虑和处理？） ABSTRACT通过keyword query对answer graphs进行摘要。 对answer graph提出summary graph的概念。保留关键词的关系，对关键词节点间的路径进行摘要。 coverage ratio——衡量摘要的信息损失。 由上一条，定义一系列摘要问题。 摘要问题的复杂度从PTIME到NP-complete。 提出exact和出去发誓摘要算法。 实验。 1. INTRODUCTION关键词查询得到太多的查询图，需要对其进行摘要，以便更好地使用。两个应用：Enhance Search with Structure. keyword queryand graph query’s usability-expressivity tradeoff.对关键词查询产生的answer table构建出结构化查询，让用户选择。Improve Result Understanding and Query Refinement.举例：三个查询结果可以摘要为两个图。本文贡献： 定义summary graph——捕捉Q中关键词和答案图之间的关系。 两个指标：摘要图大小，coverage ratio$\alpha$——衡量摘要图中关键词对占答案图中的比例。 $\alpha$-summarization problem：找符合$\alpha$的最小的摘要图。 K summarization problem：找K summary graph。 对于1-summarization，有精确解法。对于$\alpha$-summarization和K summarization 有启发式算法。 Related work 暂略。 2. ANSWER GRAPHS AND SUMMARIES2.1 Keyword Induced Answer GraphsAnswer graphs: Q的答案图是联通的无向图。$card(G)$答案图中包含的答案数量。$|G|$答案图中边和点的数量。 2.2 Answer Graph SumarizationSummary graph. 同样是无向图。 3. QUALITY MEASUREMENT3.1 Coverage MeasurementKeywords coverage. 对于答案图的并集，关键词节点$v_{k_i}$到$v_{k_j}$和摘要图Gs中$v_{s_i}$到$v_{s_j}$的路径标签一致，则覆盖了该关键词对。Coverage ratio. 定义M是被摘要图Gs覆盖的关键词对数量。 \alpha=\frac{2\cdot M}{|Q|\cdot(|Q|-1)}3.2 Conciseness MeasurementSummarization size. Gs中所有的点和边的和 3.3 Summarization ProblemsMinimum $\alpha$-Summarization. 找到规模最小的$\alpha$-summary graph。（MSUM）Theorem 1: MSUM is NP-complete (for decision version) and APX-hard (as an optimization problem).PSUM: $\alpha$为1Theorem 2: 给定Q和G，PSUM时间复杂度$O(|Q|^2)|G|+|G|^2$K Summarization. 找到概要图集合Gs，使得 集合中每个摘要图都是1-summary graph。 摘要图对应的原始答案集合是答案图集合不相交的K划分。 大小为最小，即集合中每个摘要图大小最小。4. COMPUTING $\alpha$ SUMMARIZATION4.1 Computing 1-Summary GraphsDominance relation.]]></content>
  </entry>
  <entry>
    <title><![CDATA[《Finding Patterns in a Knowledge Base using Keywords to Compose Table Answers》——论文笔记]]></title>
    <url>%2F2017%2F10%2F28%2FFinding-Patterns-in-a-Knowledge-Base-using-Keywords-to-Compose-Table-Answers-1%2F</url>
    <content type="text"><![CDATA[ABSTRACTrepresent each relevant answer as a table which aggregates a set of entities or joins of entities within the asme table scheme or pattern. A pattern is an aggregation of subtrees which contain all keywords in the texts and have the same structure and types on node/edges. 为关键字查询提供答案表（感觉就是top-k）。每个答案是有相同pattern的一群实体。基于路径的索引。两种查询处理算法，一种针对结果数较少的查询，一种针对结果较多的查询。 1. INTRODUCTION]]></content>
  </entry>
  <entry>
    <title><![CDATA[《知识图谱构建技术综述》——论文笔记]]></title>
    <url>%2F2017%2F10%2F20%2F%E3%80%8A%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0%E3%80%8B%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[因实验室的项目需要，对跨界服务知识图谱构建技术，进行相关的调研。 摘要 对知识图谱的定义和内涵进行说明，并给出构建知识图谱的技术框架，按照输入的只是素材的抽象程度将其划分为3个层次：信息抽取层、知识融合层和知识加工层。 分别对每个层次设计的关键技术的研究现状进行分类说明，指出知识图谱与相关学科的关系。 对知识图谱构建面临的挑战和问题进行总结。 背景语义网络是一张数据构成的网络，语义网络技术提供查询环境，以图形的方式返回经过加工和推理的知识。知识图谱则是实现智能化语义检索的基础和桥梁。谷歌开始。中文——知网（HowNet），百度知心，搜狗知立方，清华Xlore，中科院OpenKN，上海交大zhishi.me，复旦GDM实验室的项目。 1 知识图谱的定义与架构 1.1 知识图谱的定义 定义1. 知识图谱。是结构化的语义知识库，用于以符号形式描述物理世界中的概念及其互相关系。其基本组成单位是“实体-关系-实体”三元组，以及实体及其相关属性-值对，实体间通过关系相互连接，构成网状的知识结构。 知识图谱是一个具有属性的实体通过关系连接而成的网状知识库。点代表实体，边代表关系，由此知识图谱是对物理世界的一种符号表达。 研究价值：借助知识图谱，能够在Web网页之上建立概念间的连接关系，从而以最小的代价将互联网中的信息组织起来，成为可以被利用的知识。 应用价值：改变现有的信息检索方式，一、通过推理实现概念检索（相对于字符串的模糊匹配方式而言）；二、以图形化方式向用户展示经过分类整理的结构化知识，使人们从人工过滤网页寻找答案的模式中解脱出来。（怎么解脱？还不是太懂）1.2 知识图谱的架构 知识图谱的逻辑结构： 数据层：知识以事实为单位存储在图数据中，以三元组的方式从而形成图谱。 模式层：知识图谱的核心，存储经过提炼的知识，通常采用本地库管理知识图谱的模式层，相当于知识图谱中的模具。构建知识图谱所采用的技术架构 虚线中是构建过程，也是图谱更新的过程。详细介绍：如图所示，从原始数据出发，采用自动或半自动的方法，从数据中提取出知识，并存入知识库的数据层和模式层。这是一迭代更新的过程，每次迭代包含3个阶段：信息抽取、知识融合以及知识加工。两种方式： 自顶而下：借助百科类网站等结构化数据源，从高质量数据中提取本体和模式信息，加入到知识库中； 自底而上：借助一定的技术手段，从公开采集的数据中提取出资源模式，选择其中置信度较高的新模式，经人工审核之后，加入到知识库中。 最初自顶而下，现在流行自底而上，这也是本文介绍的重点。 2 知识图谱的构建技术2.1 信息抽取从异构数据源中抽取实体，属性和关系，并形成本体化的知识表达。 2.1.1 实体抽取早期面向单一领域： 规则：耗费人力物力，可扩展性差。 统计机器学习：单纯基于有监督学习的实体抽取方法效果不理想，且算法性能依赖训练样本的规模。 两者结合。 面向开放领域： Sekine首先对实体进行分类150种，Ling对实体进行分类112种。 但预定义分类的方式难以适应时代的需求，采用统计机器学习的方法，从目标集中抽取相似上下文进行实体的分类和聚类。 如何从少量实体示例中发现具有区分力的模式。 迭代拓展实体语料库。 通过搜索引擎的服务器日志获取新出现的命名实体。 2.1.2 关系抽取为得到语义信息，从相关预料中提取实体之间的关系。 早期人工构造语法与语义规则，不足： 1、要求制定规则的人具有良好的语言学造诣，并且对特定领域有深入的理解与认知。 2、规则制定工作量大，难以适应丰富的语言表达风格，且难以扩展到其他领域。 统计机器学习方法。 基于特征向量或核函数的有监督方法。需要大量标注语料。 半监督与无监督的学习方式。 为解决需要预先定语实体关系类型的问题： Banko提出面向开放域的信息抽取方法框架，首先从少量人工标记数据得到实体关系分类模型，然后对数据进行分类。 Wu等基于Banko的工作，利用维基百科提供的属性信息，自动构造实体关系训练集。 现在流行的OIE（Open information extraction）存在两个问题： 现在只考虑二元实体关系，很少考虑高阶多元实体关系。 只关注发掘词汇或词组之间的关系模式，而无法实现对隐含语义关系的抽取。 2.1.3 属性抽取定义：从不同信息员中采集特定实体的属性信息。 由于可将实体的属性视为实体于属性值之间的一种名词性关系，因此也可以将属性抽取问题视为关系抽取问题。 从结构化或半结构化的百科网站获取大量实体属性数据。 非结构化的公开数据。 基于百科类网站的半结构化数据，通过自动抽取生成训练许了，用于训练实体属性标注模型，然后应用于非结构化数据。 采用数据挖掘的方法直接从文本中挖掘实体属性与属性值之间的关系模式。 2.2 知识融合消除概念的歧义，剔除冗余和错误概念，从而确保知识的质量。 2.2.1 实体链接定义：对从文本中抽取得到的实体对象，将其链接到知识库中对应的正确实体对象的操作。早期：仅关注如何将从文本中抽取到的实体链接到知识库中，忽视了位于统一文档的实体间存在的语义关系。现在：利用实体的共现关系，同时将多个实体链接到知识库中——集成实体链接。 流程： 从文本中通过实体抽取得到实体指称项。 进行实体消歧和共指消解。 在确认知识库中对应的正确实体对象之后，将该实体指称项链接到知识库中对应实体。 1) 实体消歧解决同名实体产生歧义问题的技术，通常采用聚类法。聚类法是指以实体对象为聚类中心，将所有指向同一目标实体对象的指称项聚集到以对象为中心的类别下。其关键在于如何定义实体对象和指称项之间的相似度，常用方法有以下四种： 空间向量模型（词袋模型）。取当前语料中实体指称项周边的词构成特征向量，然后利用向量的余弦相似度进行比较，将该指称项聚类到与之最相近的实体指称项集合中。缺点：没有考虑上下文语义信息。 语义模型。和空间向量模型类似，区别在于特征向量构造方法不同，语义模型的特征向量不仅包含词袋向量，而且包含一部分语义特征。 社会网络模型。假设物以类聚，人以群分。建模时，首先利用实体间的关系将与之相关的指称项连接起来构成网络，然后利用社会网络分析技术计算该网络中节点之间的拓扑距离，以此来判定指称项之间的相似度。 百科知识模型。百科类网站通常会为每个实体（指称项）分配一个单独页面，其中包括指向其他实体页面的超链接，利用这种链接关系来计算实体指称项之间的相似度。但因百科类知识库中实体数有限，此类方法的推广性较差。 另一个问题如何对存在歧义的实体进行重要性评估，确定推荐内容的优先级。主要解决方法是为实体赋予权重，用于表示该实体出现的频率或先验概率。 2) 共指消解解决多个指称项对应于同一实体对象的问题。又称为对象对齐、实体匹配以及实体同义。 基于自然语言处理的方法。以句法分析为基础，代表方法是Hobbs算法和向心理论。 Hobbs算法：基于句法分析树进行搜索，适用于实体与代词出现在同一句子中。 向心理论：将表达模式视为语篇的基本组成单元，通过识别表达模式中的实体，获得当前和后续语篇中的关注中心（实体），根据语义的局部连贯性和显著性，就可以在语篇中跟踪受关注的实体。 统计机器学习。 视为分类问题。 视为聚类问题，以实体指称项为中心通过实体聚类实现指称项与实体对象的匹配。 主要缺点：训练数据的（特征）稀疏性和难以在不同的概念上下文中建立实体关联。Pantel等提出新的实体相似性测度模型——术语相似度，可从全局语料中得到所有术语间统计意义上的相似性。Chakrabarti将网页点击相似性和文档相似性相结合，提出新的查询上下文相似性测度。 2.2.2 知识合并1) 合并外部知识库 数据层的融合，包括实体的指称、属性、关系以及所属类别等，主要的温床是如何避免实例以及关系的冲突问题，造成不必要的冗余。 通过模式层的融合，将新得到的本体融入已有的本体库中。 关联开放数据项目（linked open data）会定期发布整理的语义知识数据，如DBpedia，YAGO等，Mendes等人提出开放数据集成框架（linked data integration framework，LDIF）用于对LOD只是库产品进行融合，共有四个步骤：1，获取知识。2，概念匹配。3，实体匹配。4，知识评估。确保只是图谱的一致性和准确性。 2) 合并关系知识库可以利用RDB2RDF的开源工具（Triplif, D2RServer, OpenLink Virtuoso, SparqlMap等）将关系数据库转化为RDF，从而融入到知识图谱中。 W3C推出了2中映射语言标准： Direct Mapping：采用直接映射的方式，将关系数据库表结构和数据直接输出为RDF图，在RDF图中所用到的用于表示类和谓词的术语与关系数据库中的表名和字段名保持一致。 R2RML：有较高的灵活性和可定制型，允许为给定的数据库结构定制词汇表，可以将关系数据库通过R2RML映射为RDF数据集，其中所用的术语如类的名称，谓词均来自定义词汇表。 2.3 知识加工通过信息抽取，可以从原始语料中提取出实体、关系和属性等知识要素。在经过知识融合，可以消除实体指称项与实体对象之间的歧义，得到实施表达。但事实本身并不等于知识，需要进行知识加工。 2.3.1 本体构建本体是对概念进行建模的规范，是描述客观世界的抽象模型，以形式化方式对概念及其之间的联系给给出明确定义。 人工编辑的方式手工构建（借助本体编辑软件）。 数据驱动的方式自动构建，然后采用算法评估和人工审核结合的方式加以修正和确认。（主流） 实体并列关系相似度计算：考察两个实体在多大程度上属于同一概念分类。 模式匹配法：预先定义实体对模式，通过模式匹配取得给定关键字组合共现频率，计算相似度。 分布相似度法：假设相似的上下文中频繁出现的实体语义相似，将每个实体表示为N维向量，每个维度表示预先定义的上下文环境，向量元素值表示尸体出现在各上下文环境中的概率，然后通过求解向量间的相似度，得到实体的并列关系相似度。 实体上下位关系抽取：用于确定概念之间的隶属（IsA）关系。 基于语法模式出去IsA实体对。 基于语义的迭代技术。利用概率模型判定IsA关系和区分上下位词，通常会借助百科类网站提供的概念分类知识来帮助训练模型。 本体生成：对各层次得到的概念进行聚类，并对其进行语义类的标定。 实体聚类方法，难点在于经信息抽取得到的实体描述非常简短，缺乏必要的上下文信息，导致许多统计模型不可用。 2.3.2 知识推理 基于逻辑的推理 一阶谓词逻辑，建立在命题（个体&amp;谓词两部分）的基础上。适用于简单关系。 描述逻辑，基于对象的知识表示的形式化工具，是一阶谓词逻辑的子集。借助TBox（terminology box）和ABox (assertion box)，将基于描述逻辑的推理最终归结为ABox的一致性检验问题，从而简化并最终实现关系推理。 基于规则的推理。因在描述属性合成和属性值转移方面，网络本体语言的表达能力显得不足，为实现推理，可利用规则语言（如，SWRL）对本体模型添加规则进行功能拓展。 基于图的推理方法 神经网络模型 Path Ranking算法 加入知识库前需要进行可证明性检查、矛盾性检查、冗余性检查以及独立性检查。此外跨知识库的知识推理也是大趋势，如卢道设等的工作。 2.3.3 质量评估原因： 受现有技术技术水平的限制，采用开放域信息抽取技术得到的知识元素可能存在错误，知识推理得到的知识也没有保障。 随着开放关联数据项目的推进，子项目产生的知识库产品间的质量差异也在增大，数据间的冲突日益增多，如何对其质量进行评估，对全局知识图谱构建起着重要作用。 意义：对知识的可信度进行量化，通过舍弃置信度较低的知识，保障知识库的质量。举例：Mendes对LDIF提出的Sieve方法，Fader对REVERB系统提出的逻辑斯蒂回归模型，谷歌等的例子。 2.4 知识更新两方面： 概念层的更新：添加新概念。人工审核。 数据层的更新：新增或更新实体、关系和属性值。考虑数据源的可靠性、数据的一致性。选择百科类网站等可靠的数据源，并选择个数据源中出现频率高的事实和属性加入知识库，也可以采用众包的模式。 两种方式： 数据驱动下的全面更新。方式简单，但资源消耗大，需大量人力进行系统维护。 增量更新，资源消耗小，但需要大量人工干预，实施起来困难。 3 跨语言知识图谱的构建意义： 弥补单语种知识库的不足。 利用多语种在知识表达上的互补性，增加知识的覆盖率和共享度。 可比较不同语言对统一制式的表述，来过滤错误信息，更新过时信息。 关键问题有三个，跨语言本体的构建，见2.3.1节。其他两个如下。 3.1 跨语言知识抽取欧盟的Xlike和我国的Xlore项目。思路是借助于丰富的源语种知识自动化抽取缺失的目标语种知识。 3.2 跨语言知识链接 模式层的链接：核心是本体映射，如果两个本体间存在语义上的概念关联，则通过语义关联实现二者之间的映射，本体映射的目的是实现知识的共享和重用。 数据层的链接。 距离Wang等的基于链接因子图模型的跨语言知识链接方法，基于链接相似度方法。 4 知识图谱的应用智能语义搜索、移动个人助理以及深度问答系统。基于知识图谱的问答系统可分为2类： 基于信息检索的问答系统。把问题转变为基于知识库的结构化查询。 基于语义分析的问答系统。首先通过语义分析正确理解问题的含义，然后将问题转变为知识库的精确查询。 5 问题与挑战 信息抽取环节，面向开放域的信息抽取方法研究还处于起步阶段，部分研究成果虽在特定数据集上缺德较好结果，但限制多，扩展性不好。 知识融合环节。如何实现准确的实体链接是一个主要挑战。 知识加工是最具特色的知识图谱技术，同时也是该领域最大的挑战之所在。主要研究问题：本体的自动构建、知识推理技术、知识质量评估手段以及推理技术的应用。 知识更新环节，如何确保自动化更新的有效性是一个重大挑战。 如何解决知识的表达、存储与查询问题。 6 结束语互联网正从包含网页和网页之间超链接的文档万维网转变为包含大量描述各种实体和实体之间丰富关系的数据万维网。知识图谱作为下一代智能搜索的核心关键技术，具有重要的理论研究价值和现实的应用价值。本文从知识图谱构建的视角，对知识图谱的内涵，以及知识图谱构建关键技术的研究发展现状进行了全面调研和深入分析，并对知识图谱构建工作面临的重要挑战和关键问题进行了总结。知识图谱的重要性不仅在于他是一个全局知识库，是支撑智能搜索和深度问答等智能应用的基础，而且在于他是一把钥匙，能够打开人类的知识宝库，为许多相关学科领域开启新的发展机会。从这个意义上来看，知识图谱不仅是一项技术，更是一项战略资产。本文的主要目的是介绍和宣传这项技术，希望吸引更多人重视和投入这项研究工作。]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>语义网</tag>
        <tag>自然语言处理</tag>
        <tag>语义搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《TrieJoin: Efficient Triebased String Similarity Joins with EditDistance Constraints》——读书笔记]]></title>
    <url>%2F2017%2F10%2F18%2F%E3%80%8ATrieJoin-Efficient-Triebased-String-Similarity-Joins-with-EditDistance-Constraints%E3%80%8B%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[虽然师兄说，看PPT就行了，但是我粗略一看PPT感觉说的貌似太简略了，还是得看一下论文。。。应该是水平太低所以看PPT没有深入理解吧，希望这次看论文能快点！（有选择的看） ABSTRACT主要工作：使用编辑距离研究字符串相似。现有方法使用filter-and-refine框架，缺点如下： 对长度小于30的短字符串效果不好。 巨大的索引。 数据集更新的成本很大。 本文使用trie-join框架，能用小的索引生成结果。使用trie结构索引字符串，并使用trie剪枝找到相似子串。并且支持数据集的动态更新。 1.INTRODUCTIONsimilarity join简介。相似函数：jaccard similarity，cosine similarity，edit distance。举例之前工作，并指明缺点（见摘要）。本方法解决了之前的所有问题。贡献： We propose a trie-based framework for efficient string similarity joins with edit-distance constraints. We devise efficient trie-join-based algorithms and develop pruning techniques to achieve high performance. We extend our method to support dynamic update of data sets efficiently. 2.TRIE-BASED FRAMEWORK2.1 Problem Formulation如果两个字符串编辑距离小于阈值，则两个字符串相似。 Definition 1 (String Similarity Joins). Given two sets of strings R and S, and an edit-distance threshold $\tau$, a similarity join finds all similar string pairs $&lt; r,s&gt; \in R \times S$ such that $ED(r,s)\le \tau$ 2.2 Prefix Pruningnaive solution：计算所有字符串对的编辑距离。使用动态规划提早终止编辑距离的计算。string $r=r_1,r_2…r_n$, $s=s_1,s_2…s_m$,matrix D(i,j): the edit distance between the prefix $r_1,r_2…r_i$ and $s_1,s_2…s_j$D(0,j)=j for $0\le j\le n$, andD(i,j)=min(D(i-1,j)+1,D(i,j-1)+1,D(i-1,j-1)+$\theta$) where $\theta=0$ if $r_i=s_j$; otherwise $\theta=1$D(i,j) is an active entry if $D(i,j)\le \tau$如图没有active entries时，结束搜索。 2.3 Our ObservationObservation 1 - Subtrie Pruning:通过前缀可以剪枝掉很多字符串。 Lemma 1 (Subtrie Pruning). Given a trie T and a string s, if node n is not an active node for every prefix of s, then n’s descendants will not be similar to s. Trie-Search: 首先为R内的所有字符串构建trie树，然后基于subtrie pruning对S中的每个字符串计算active-node set $A_s$。 Observation 2 - Dual Subtrie Pruning:Subtrie Pruning只利用了R中的trie结构。对R和S中的字符串构建了一个大的Trie，同时对R和S进行subtrie pruning。 Lemma 2 (Dual Subtrie Pruning). Given two trie nodes u and v, if u is not an active node for every ancestor of v, and v is not an avtive node for every ancestor of u, the strings under u and v connot be similar to each other. 但利用dual trie pruning，不能直接在树中搜索找到相似对。 3. TRIE-BASED ALGORITHMS为便于表示，关注自连接，即R=S。 3.1 Trie-Traverse AlgorithmAlgorithm Description：首先为S中的字符串构建trie索引，然后先序遍历，对于每个节点，计算其active-node set，到达叶子结点，则和active-node set中的叶子结点构成相似对。Computing Active-Node Sets: 利用其父节点的active-node set $A_p$，计算本节点active-node set $A_n$，$A_n$中的每个点都能在$A_p$中找到父节点。 Lemma 3. Given a node n, let p denote n’s parent, for each node $n^{‘}\in A_n$, there must exist a node $p^{‘}\in A_p$, such that $p^{‘}$ is an ancestor of $n^{‘}$. 因为从$A_p$计算$A_n$的复杂度为$O(\tau \cdot |A_n|)$，所以Trie-Traverse的时间复杂度为$O(\tau \cdot |A_T|)$ 3.2 Trie-Dynamic Algorithm利用active nodes的对称性：if u is an active node of v, then v must be an active node of u。减少冗余的计算。Trie-Dynamic: 动态的构建trie结构，每次插入新的节点，计算在当前trie结构上新节点的active-node set，并基于对称性更新其他节点的active-node set。时间复杂度下降为1/2，但需要维护所有点的active-node set 使得空间复杂度升高。 3.3 Trie-PathStack AlgorithmTrie-Traverse：内存占用更小但有一些冗余的active-node计算。Trie-Dynamic：避免了重复计算但使用了更多的内存。Trie-PathStack解决了上述问题。首先，维护一个”virtual partial” subtrie保存所有访问过的节点。对于每个未访问的节点，首先设置为已访问，然后计算”virtual partial” subtrie中的active-node set。然后，先序遍历trie节点，并用栈保存需要被更新的节点。在先序遍历时，用栈保存从根到当前节点路径上的所有节点，访问当前节点时，其父节点一定在栈顶，利用父节点active-node set计算当前节点的active-node set，计算后只需更新栈中最多$\tau$个节点。主要是对Trie-Tranverse的改进，不是对整个图进行遍历，而是对已访问的部分进行遍历，然后利用栈保存路径。（这里不太懂，为啥要保存路径呢。。。） 4. PRUNING TECHNIQUESLength Pruning: 长度相差大于$\tau$直接剪枝。Single-branch Pruning: 在同一个分支上，如果其叶子节点相同，则父节点可被剪枝。Count Pruning: 如果两个点只能生成一个字符串，则可以删掉一个字符串。 5. INCREMENTAL SIMILARITY JOINS Definition 2 (Incremental Similarity Joins). Given a set of strings S, a new string set $\Delta S$, and an edit-distance threshold $\tau$, an incremental similarity join finds all similar string pairs $(r\in \Delta S, s\in S\cup \Delta S)$ such that $ED(r,s)\le \tau.$]]></content>
  </entry>
  <entry>
    <title><![CDATA[《Top-k Exploration of Query Candidates for Efficient Keyword Search on GraphShaped RDF Data》——读书笔记]]></title>
    <url>%2F2017%2F09%2F29%2F%E3%80%8ATop-k-Exploration-of-Query-Candidates-for-Efficient-Keyword-Search-on-Graph-Shaped-RDF-Data%E3%80%8B%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[首先，初步确定一下本周的规划，今明两天读本篇论文，以及回顾之前的论文（按照师兄说的方式进行泛读），然后周三周四学习斯坦福的课程。周五将没有整理的论文整理完全，周六日搞定本篇和上篇论文。 Abstract从关键词中计算出queries之后，让用户选择合适的，然后送入数据库引擎进行查询。另外对于queries的计算，提出了找top-k子图的新型算法。 1. Introduction提出关键词这样的搜索一直以来都是研究的重点。Labelled query model：不需要用户对结构有任何的了解，只是单纯的将关键词和“labels”联系起来。现在关键词搜索的主要思路：将关键词映射到data elements（很多使用精确匹配），搜索连接关键词的elements的子图（如Blinks中的distinct root assumption），基于得分函数（被提出了很多，从路径长度到IR中的复杂度量）输出top-k个子图（需要计算每个选项的上限和下限）。任务可分为四个： keyword mapping. graph exploration. scoring. top-k computation. 本方法结合了语法和语义相似度，因此IR概念支持模糊匹配。本文贡献如下： Keyword Search through Query Computation 将关键词转化为结构化查询的元素（而不是答案的一部分），让用户选top-k个查询中的一个（而不是直接把top-k个答案给出）。 Algorithms for Subgraph Exploration 当前的方法通常将关键词映射到节点，算法计算出树形的答案。但关键词也可能映射到边，所以答案结构也不一定是树。 Efficient and Complete Top-k through Graph Summarization 很难簿记计算top-k所需的信息，现在的方法不能确保结果是真正的top-k。因此我们引入了复杂的数据结构保存所有候选的得分。为了效率，利用摘要图进行剪枝。 2. Problem DefinitionDataQueries用户的查询$Q_U$就是关键词集合，系统的查询$Q_S$是conjunctive queries.Answers就是把查询中的distinguished variables 替换成子图中的节点。Problem主要考虑conjunctive queries的计算。 3. Overview of the Approach首先举例几个关键词，指出他们之间的连接需要推断关键词之间的连接。以往的工作使用图的schema来推断。本文的方法如下：Query Computation keyword通过映射得到keyword elements。 通过图的搜索将keyword elements连接起来找到connecting element。 connecting element与keyword elements之间的路径构成了matching subgraph。 对于每个子图，通过graph elements到query elements的映射得到conjunctive query。Preprocessing预处理得到keyword index，用于keyword-to-element映射。为了图搜索，建立graph index——原始图的摘要。the augmented index 可以推导query中谓词、常量和结构。Running Example 4. Indexing Graph DataA. The Keyword Index关键词可能指C-vertices，E-vertices，V-vertices或 edges，但在构建索引时忽略E-vertices因为用户不太可能直接输入E-vertex的URI。keyword index 就是一个keyword-element映射，但是对于V-vertex和A-edge所存储的结构比较特殊。为了识别不予数据元素的标签严格匹配的关键词，keyword-element 实现为一个倒排列表。首先对labels进行分析得到其terms，然后利用WordNet得到terms的同义词，上位词和下位词。所以语义相似的graph element会被检出，并用Levenshtein距离度量keywords到terms的语法相似性。 B. The Graph Schema Index用作搜索连接keyword elements的子结构。之前的文章在全图进行搜索，本文旨在从边推出查询结构，从点推出常量（变量）。A-edges和V-vertices并不会有助于连接keyword elements，除非他们就是keyword elements。 构建$G^{‘}_K$需要利用来自映射的数据结构，即[V-vertex, A-edge, $(C-vertex_1,…,C-vertex_n)$]和(A-edge, C-vertex)。为了关键词能够匹配V-vertex，将A-edge$(C-vertex_i,V-vertex)$加入$G^{‘}$;为了关键词能够匹配A-edge，将A-edge$(C-vertex_i,Value)$加入$G^{‘}$. 5. Scoring介绍了一些得分函数，如PageRank（为节点打分），最短路径（为路径打分），TF/IDF（为keyword element打分）。对于图来说，其由路径构成，其成本函数如下： C_G=\sum_{p_i\in P}C_{p_i}而路径由其elements组成： C_{p_i}=\sum_{n\in p_i}c(n)Path Length 假设用户所需的实体紧密相连。其得分函数为C_1=\sum_{p_i\in P}\sum_{n\in p_i}1Popularity Score 计算摘要图中element的popularity，越流行则在路径中贡献越小。 C_2=\sum_{p_i\in P}\sum_{n\in p_i}c(n)，其中对于点v，c(v)=1-\frac{|v_{agg}|}{|V|}，对于边e，c(e)=1-\frac{|e_{agg}|}{|E|}。|V|：摘要图中点的总数。 v_{agg}：graph index 中聚集在一个C-vertex的E-vertex的数量。|E|：摘要图中边的总数。 e_{agg}：摘要途中聚集在一个R-edge的R-edge的数量。Keyword Matching Score C_2=\sum_{p_i\in P}\sum_{n\in p_i}\frac{c(n)}{S_m(n)} $S_m(n)$代表element n的得分，对于Keyword element，范围是[0,1]，其他元素则一律设置为1。其从语法语义两方面考虑，得分越高则路径的成本越小。 前两个可以离线计算，因为element在不同路径的话，会计算多次，所以其更倾向于Keyword elements紧密连接的子图。 6. Computation of Queries对于查询计算，有五个任务： mapping of keywords to data elements. augmentation of the summary graph. exploration of the graph to find subgraphs connecting the keyword elements. top-k processing. generation of the query for the top-k subgraphs前两个已经解决，本节解决3-5. A. Algorithms for Graph Exploration首先定义最小匹配子图：一个条件定义包含所有关键词，另一个确保联通。与现有的搜索算法进行对比。Backward Search 从Keyword elements出发迭代地沿入边访问elements直到找到一个connecting element，即answer root。Bidirectional Search 该方法认为从一些顶点可以通过跟随传出而不是传入边缘来更快地达到答案根。故使用启发式激活因子来估计边缘将到达answer root的可能性。这些因子是从一般图形拓扑和已经探索的元素得出的。虽然其在很多情况下表现很好，但最差性能无法保证。Searching with Distance Information 通过存储在索引中的附加连接信息保证最差性能是m-optimal。在每次迭代中，通过该信息可确定能够达到keyword element的elements以及最短的距离，从而有目标的进行搜索。不过构建这些信息十分费力。 因为关键词也有可能对应边，所以查询出的结果不再是树，而是图。成本来自于两方面：query-independent，query-specific。索引技术只能解决query-independent的成本。 B. Search for Minimal Matching Subgraph Input and Data Structures G^{'}_K：摘要图 K=(K_1,...,K_m)：keyword elementsk：查询数量c(n,k,p,d,w): n 刚访问的graph element，k c所在路径起点的keyword element，p 父游标，d 距离，w 成本。$LG^{‘}$: 保存候选子图的全局变量。$K_{lowC}$： 存储成本最低的keyword element。 Initialization and General Idea 从一系列keyword elements出发，为每个查询创建游标，游标的拓展就是搜索的拓展。Garph Exploration需要注意邻居可能是出边，入边和点。Computation of Distinct Paths解决环形的问题。Termination 一项被满足 已经计算出所有可能的不同路径，使得LQ中没有更多的游标。 所有keyword elements在给定长度内的所有路径被搜索。 top-k查询被计算。 C. Top-k Computation 基本思想来自TA（Threshold Algorithm）算法。候选子图的最高成本——下限的计算，其余子图的最低成本——上限的计算如下：Candidate Subgraphs element n如果能达到所有关键词（其每个关键词游标都不空），则可能对应多个子图（每个游标可能有多个路径），计算每个子图的成本并排序。Remaining Subgraphs 和其他方法相比，我们首先支持图，不限于树。不止是距离信息，还设置了多样的成本函数。首先对这些信息（那些信息）进行索引可以提高Top-k处理和图搜索的效率。In our approach, minimality can be guaranteed for any score metrics, given that the scoring function is monotonic.和【1】对比。时间复杂度$|G|^{d_max}$空间复杂度$k\cdot |K|\cdot |G|$ D. Query Mapping将子图映射到conjunctive query。Processing of Vertices constant(v) 返回点v的label，var(v)返回v代表的变量。Mapping of A-edgesMapping of R-edges认为相同根的不同答案树是有价值的。将所有答案呈现给用户， 让用户选择。 7. Evaluation基于关键词查询，计算出top-k个conjunctive queries，转化成自然语言问题，并展现给用户。数据集：DBLP、TAP、LUBM。 A. Effectiveness Study12人DBLP—30查询 TAP—9查询使用Reciprocal Rank（RR） =1/r。r是正确查询的排名。 B. Performance Evaluation对比算法：bidirectional search，1000 BFS，1000 METIS， 300 BFS，300METIS。Comparative Analysis query computation的时间，query processing的时间。实验中总时间=计算top-10的时间+处理查询直到找到至少10个答案的时间。Search Performance k 的影响——线性。查询长度Index Performance 索引大小及建索引的时间，都可以接受。 8. Related Worknative approaches：直接在图结构数据上进行关键词搜索，虽然schema-agnostic，但是需要特定的目录和存储机制。Database extensions：可以利用底层数据库的机制，如DBXplorer，Discover。用schema中的信息连接构建的表达式，从而将关键词转成候选网络，再将候选网络转成SQL查询。本方法结合两种方法的优点，一、schema agnostic，构建了schema，并在schema上进行搜索。二、可以利用底层RDF存储的机制。之前工作：计算得出答案，将关键词映射到三元组。本方法：计算得出top-k查询，将关键词映射到查询的element（这样可以支持更多pattern）。前向和后向搜索会利用索引存储关键词信息和路径信息，本方法虽然也用关键词和距离索引，但只是为了计算分数。之前方法计算distinct trees，本方法计算一般子图，因此需要遍历所有的入边和出边。本方法通过预留的索引信息在guided exploration下可以得到最佳的得分，离线部分用索引计算，在线部分用TA计算。但其他方法并不能为结果提供top-k保证。 9. Conclusion and Future Work]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Keyword</tag>
        <tag>2017年9月</tag>
        <tag>RDF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客编写]]></title>
    <url>%2F2017%2F09%2F27%2Fhexo%E7%BC%96%E5%86%99%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[需参考网站：https://github.com/zhaoqingqing/zhaoqingqing.github.io 昨儿晚上和今天下午都在搞我的博客，本来想搞多终端同步，结果莫名其妙的hexo-admin的deploy功能不能使用了，然后重装了好几次，都没有成功，心里很烦躁。现在冷静下来之后决定用命令去deploy，然后得空去学习js，看看这些东西怎么操作吧。 博客的编写发布主要参考 https://righere.github.io/2016/10/10/install-hexo/首先把hexo分支git clone到本地，然后npm install安装hexo。编辑blog还是用hexo-admin然后使用git add . git commit -m “改了啥”， git push origin hexo将本地仓库同步到远程发布使用命令hexo d -g。 其他终端先pull，之后进行接下来的操作。心好累，弄了半天还是没有什么结果。先暂且这么用着吧。 10月8日 数学公式有问题，按照http://xudongyang.coding.me/math-in-hexo/ 修改。]]></content>
      <categories>
        <category>关于博客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《Answering top-K query combined keywords and structural queries on RDF graphs》——读书笔记]]></title>
    <url>%2F2017%2F09%2F04%2F%E3%80%8AAnswering-top-K-query-combined-keywords-and-structural-queries-on-RDF-graphs%E3%80%8B%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ABSTRACT虽然SPARQL是RDF图上优越的查询语言，但一些查询意图仍无法使用SPARQL句法表达。关键词搜索虽然能够直观表示信息的需求，但表达准确度较低。为了综合两者的优点，提出了混合查询SK query，并使用基于结构化索引的新型查询算法加速查询。为了更进一步提高SK查询的效率还使用了基于距离的优化技术。 1. Introduction目前RDF图十分流行，图1是Yago知识图谱的一个例子。SPARQL基于子图匹配，是查询RDF数据的标准方法。但是由于用户不了解RDF的schema，所以查询的实体和谓词并不能和数据库中対映。关键词查询是说明信息需求更直观的方法，但也常得到一些无意义的答案。因此本文结合两者的优点提出SK query，其结果是最接近所有关键词的k个SPARQL结果。我们假设关系的强度依赖于路径长度，另外不同的谓词也应该有不同的权重。该问题的另一个挑战是搜索效率，穷举法的流程如下： 用现有技术找出所有匹配的子图。 计算匹配子图和包含关键词的点间的最短路径。 找到路径最短的作为答案。 但该方法太低效，我们为SPARQL查询Q设计了一个下限以尽早结束搜索，另外为结构化剪枝提出一个星型索引。提出一个基于距离的优化加速最短路径距离的计算——选择一些中心点，并使最短路径树以这些点为根；如果搜索到了中心点p，则可以使用根在p的最短路径减少搜索空间。本文贡献： 提出一个新的查询模式——SK query，结合了SPARQL和关键词，并提供了解决方法。 提出星型索引并实现最短路径树（基于距离的优化）以减少搜索空间和提高查询性能。 实验。 2. Background2.1 Preliminaries Definition 2.1. An RDF data graph G is denoted as &lt; V(G), E(G), L&gt;, where (1) $V(G)= V_L \cup V_E \cup V_C$ is the set of vertices in RDF graph G ($V_L,\, V_E,\, V_C$ denote literal, entity and class vertices); (2) E(G) is the set of edges in G; and (3) L is a finite set of edge labels, i.e. predicates.Definition 2.2. An SK query is a pair &lt; Q,q&gt;, where Q is a SPARQL query graph, and q is a set of keywords $\{w_1,w_2,…,w_n\}$. 对于SK query &lt; Q,q&gt;，查询结果是$&lt; M,\{ v_1,v_2,…,v_n\} >$, 其中M是Q的子图匹配，$v_i$是包含关键词$w_i$的literal vertex。 Definition 2.3. Given a result $r=&lt; M,\{v_1,v_2,…,v_n\}&gt;$, the cost of r is defined as follows: Cost(r) =Cost_{content}(r)+Cost_{structure}(r).Definition 2.4. Given a result r=< M,\{v_1,v_2,...,v_n\}>, the content cost of r is defined as follows: Cost_{content}(r)=\sum^{i=n}_{i=1}C(v_i,w_i),where $C(v_i,w_i)$ is the matching cost between $v_i$ and keyword $w_i$. 结构成本只考虑SPARQL查询中的变量——理由: 用户更感兴趣。（我感觉这并不科研） Definition 2.5. Given a result r=< M,\{v_1,v_2,...,v_n\}>, the distance between match M and vertex v_i is defined as follows: d(M,v_i)=MIN_{v\in M}\{d(v,v_i)\}其中v是M中和SPARQL查询中某个变量相关的点$ d( v, v_i ) $是v和G中$v_i $的最短距离。结果r的结构成本：$Cost_{content}(r)=\sum^{i=n}_{i=1} C(v_i,w_i) $ (Problem Definition) Given an SK query &lt;Q,q&gt; and parameter k, our problem is to find the k results that have the k-smallest costs. 2.2 Predicate salience本文使用最短路径距离评估关系强度。一般的最短路径距离不区分谓词，把”type”、”label”等和普通谓词同等看待不合理。因此引入了predicate salience： ps(p)=\frac{|V(p)|}{|V(G)|} 3. Overview Keyword Mapping. 离线时，为每个关键词建立倒排列表。在线时，根据倒排列表获取关键词对应的节点。对于关键词节点，在给定查询上的常用度量是TF/IDF成本。参考文献中有很多成本函数，我们选择其中一种计算包含关键字的节点的成本。本文中主要关心如何找到SPARQL的匹配以及和关键词之间的关系。我们使用现有的IR引擎分析给定的关键词，并执行不精确匹配得到一些语法或语义相似的元素。Candidate Generation. 如果找到能到达所有关键词的节点，则需要使用子图同态检查SPARQL的子图匹配是否包含该点。此步采用“filter-and-refine”策略，首先找到一些没有在任何Q的子图匹配中出现的dummy节点，如果搜索到dummy节点则不执行子图同态。本文提出一种frequent star pattern-based structural index。基于该索引可以为SPARQL查询的变量提供候选列表。Top-k Results Computation. 基于图搜索，循环地计算关键词节点与邻居的距离，找到一个能达到所有关键词的节点，如果不是dummy vertex，则使用SPARQL matching算法。 4. Candidate generation based on the structural index4.1 Structural index本节提出一个frequent star pattern-based index。从G中挖掘出一些常见的星型模式，并为每个星型模式建立一个节点的倒排列表。选择星型的原因是在SPARQL查询常包含星型子查询。如表2，每个实体的谓词以字典序存储，如图6，sequential pattern和star pattern一一对应，使用现有的sequential pattern挖掘算法，如PrefixSpan来挖掘星型模式。 我们不能为每个星型模式建立目录，因此我们我们定义了discriminative ratio。L(S)={v|S occurs in v’s adjacent edge seuence}即返回符合星型模式S的所有点v。 Definition 4.1. Given a star S, its discriminative ratio is defined as follows:$\gamma (S)=\frac{|L(S)|}{|\cap_{S^{‘}\subset S} \,\,L(S^{‘})|}$ 如果$\gamma (S)$越大，则说明如果保存S的子集作为目录元素的话，就没必要保存S作为目录元素。因此设定$\gamma (S)\le \gamma_{max}$。但对于只有一条边的星型查询，我们始终将其放入目录中。 Theorem 4.1. Let F denote all selected index elements (i.e., frequent star patterns). Given a SPARQL query Q, a vertex v in graph G can be pruned (there exists no subgraph match of Q containing v) if the following equation holds.$v\notin \cup_{S\in F \land S \in Q}L(S)$,where $S\in F$ means that S is a selected star pattern and $S\in Q$ is a star pattern included in Q.定义剪枝策略，如果该点不在查询的子结构中，则剪枝。 4.2 Candidate generation先依据Theorem 4.1剪枝掉不可能的点，然后根据变量的predicate sequence在索引找中找到候选点。 Definition 4.2. Dummy Vertex. Given a SPARQL query Q, a vertex v in graph G is called a dummy vertex if the following equation holds.$v\notin \cup_{S\in F \land S \in Q}L(S)$,where F denotes all selected frequent star patterns, $S\in F$ means that S is a selected star pattern and $S\in Q$ is a star pattern included in Q. 搜索时，如果v不是dummy vertex，则执行子图同态算法找到包含v的SPARQL查询的匹配结果。 5. Top-k results computation5.1 Graph exploration目的是找到图中与关键词节点相联系的节点并计算距离。 Definition 5.1 Distance between a Vertex and Keyword. Given a vertex v in RDF graph G and a keyword $w_i$, the distance between v and keyword $w_i$ (denoted as $d(v,w_i)$) is the minimum distance between v and a vertexin $V_i$, where $V_i$ includes all literal vertices containing keyword $w_i$ in G. 对每个关键词遍历，然后对于关键词k所在的优先队列使用bfs，搜索邻居，并更新相关变量。 Theorem 5.1. When a queue head (v,p,|p|) is poped from queue $PQ_i$, the following equation holds.$d(v,w_i)=d[v][i]=|p|$ Definition 5.2. Seen by a Keyword. When queue head (v,p,|p|) is popped from queue $PQ_i$, the distance between v and keyword $w_i$ has been computed. We then say that vertex is seen by keyword $w_i$.**Definition 5.3. Fully Seen Vertex, Partially Seen Vertex, and Unseen Vertex. 5.2. Generatinon of SPARQl matches找到fully seen vertex v，下一步是计算包含v的SPARQL匹配。如算法2所示，我们使用基于DFS的状态转移算法，从fully seen vertex v开始进行匹配处理，对于已经处理过的节点v，因为包含该点的答案都已经得到，所以在处理其他节点时，跳过该点，如图8。 Definition 5.4. Given a SPARQL query graph Q with m vertices $u_1,…u_m$, a state is a (partial) match of query graph Q. 5.3. Top-k computation把所有节点计算后，然后取出top-k的答案，太低效，本节设计了early-stop strategy。 Definition 5.5. Fully Seen Match, Partially Seen Match and Unseen Match. Given a subgraph match M of SPARQl query Q, if all vertices in M are fully seen vertices, M is called a fully seen match; if M is not a fully seen match and M contains at least one fully seen vertex, it is called a partially seen match. If a match M does not contain any fully seen vertex, it is valled an unseen match. early-stop strategy：只计算fully seen mateches 的成本，以k-th小的成本作为阈值$\delta$并且计算partially seen matches和unseen matches的下限$\theta_1$和$\theta_2$，当且仅当$\delta &lt;\theta_1 \land\delta &lt;\theta_2$时，提前停止。否则，继续下一轮迭代。 Fully Seen Match.按照Definition 2.5计算成本，并维护阈值$\delta$。 Partially Seen Match.按以下方式计算下界。 Theorem 5.2. Given a partially seen match M of SPARQL query Q, v is a partially seen or an unseen vertex in the match. The following equation holds.$Cost(M)=\sum_{1\le i\le n}d(v,w_i)\ge \sum_{d[v] [w_i]\ne null\land 1\le i\le n}d[v] [w_i] +\sum_{d[v][w_i]= null\land 1\le i\le n}|p_i|$where $[v][w_i]$is the i-th dimension of v’s vector corresponding to keyword $w_i$, and $|p_i|$ corresponds to the current queue head $(v,p_i,|p_i|)$ in queue $PQ_i$ 依据Theorem 5.2，a partially seen match M的下界为： Definition 5.6 Given a match M of SPARQL query Q, the lower bound of a partially seen match M is defined as follows.$lb(M)=MIN_{v\in M} (\sum_{d[v] [w_i]\ne null\land 1\le i\le n}d[v] [w_i] +\sum_{d[v][w_i]= null\land 1\le i\le n}|p_i|)$ 所有partially seen matches的下界为： Definition 5.7 The lower bound $\theta_1$ for all partially seen matches is as follows.$\theta_1=MIN_{M\in PS}(lb(M))$where PS denotes all partially seen matches. 在迭代过程中，一些partially seen matches变成了fully seen matches，阈值$\delta$和$\theta_1$也随之更新。 Unseen Match.包含两种点：Partially seen Vertex、 Unseen Vertex. Theorem 5.3. For an unseen vertex v, if threshold $\delta\ne \infty$, the following equation holds.$\delta \le \Sigma_{1\le i\le n}d(v,w_i)$ 按照Theorem 5.3，unseen match的下限没必要考虑unseen matches。 Definition 5.8. The lower bound $\theta_2$ for all unseen matches is as follows.$\theta_2=MIN_{v\in PSet}(\sum_{d[v] [w_i]\ne null\land 1\le i\le n}d[v] [w_i] +\sum_{d[v][w_i]= null\land 1\le i\le n}|p_i|),$where PSet contains all partially seen vertices, $d[v][w_i]$ is the i-th dimension of v’s vector corresponding to keyword $w_i$ and $|p_i|$ corresponds to the current queue head $(v, p_i, |p_i|)$ in queue $PQ_i$. Early-stop Strategy.在每次迭代中，都检查$\delta\le \theta_1 \land \delta \le \theta_2$.如果条件成立，则算法停止。 6. Distance-based optimization算法1使用后向搜索遍历RDF图，为加快搜索，我们提出了pivot-based distance index. 离线时选中一些点作为pivots，预先计算根在pivot的路径，搜索时如果遇到pivot，则利用预存的距离信息，减少搜索空间。 6.1 Pivot-based search for top-k results of SK queries Definition 6.1. Given a shortest path tree T rooted at vertex r (denoted as T(r)), pivot p, and vertex v, if the shortest path between r and v crosses pivot p, we say that v is covered by p in T.Theorem 6.1. if v is covered by p in the shortest path tree T(r), d(r,v)=d(r,p)+d(p,v) where d(r,v) denotes the shortest path distance between r and v. 如图10，T(015)中的点001被pivotcover，d(015,001)=d(015,017)+d(017,001)=2.222. 7-17行和18-20行与算法1的对比。 6.2 Pivot selection Definition 6.2. Given a shortest path tree T(v) rooted at v and a set of pivots PV, the covered ratio is cr(T(v))=\frac{|\{v^{'}|v^{'}\,\, is\,\,covered\,\, by\,\, p\,\, in\,\, T\,\,and\, \,p\,\, \in\, \,PV\}|}{|V(G)|}Theorem 6.2. Given a constant M, finding a pivot set PV to maximize $(\sum_{v\in V(G)}cr(T(v)))$ is a NP-hard problem, where |PV|=M and T(v) denotes the shortest path tree rooted at v. 因此使用一些启发式选择pivots——a high-degree strategy. 6.3 Further optimization后期大部分路径都被计算出来，没必要再去load pivot的最短路径树，因此提供了一个终止loading的条件。$Cost_{update}=Count_{update}\times Cost_{CPU}$where $Cost_{CPU}$ is the average CPU cost of a distance update operation.$Cost_{I/O}$ is the average I/O cost of a distance update operation.1) if $Cost_{update}\le Cost_{I/O}$，continue to load pivot’s shortest path tree.2) if $Cost_{update}&gt; Cost_{I/O}$，end loading pivot’s shortest path tree.其中$Cost_{CPU}$和$Cost_{I/O}$是常数。 7. Experiments数据集：DBLP,Yago,DBPedia.Baseline: (Effectiveness)BANKS,BANKS,Annotated SPARQL.（分别对应三个数据集） (Efficiency) exhaustive computing（第一节有提到）, Basic Search(Algorithm 1), Pivot-based Search(Algorithm 3) 7.1 Datasets and setupOur experiments were conducted on a machine with a 2.4 Ghz Core 2 Duo processor and 80G RAM memory. 7.2 Effectiveness study7.2.1 Case Study 7.2.2. NDCG@k over Yago and DBLPNDCG——the normalized discounted cumulative gain。 7.2.3. MAP over Yago and DBLPMAP——the mean average precision. 7.3. Efficiency study7.3.1. Pruning effect of the structure index. 7.3.2. Evaluation of pivot selection methods7.3.3. Evaluation of pivot numbers7.3.4. Offline performance7.3.5. Online performance8. Related work SPARQL查询。一些把RDF存储在RDBMS，用join操作回应SPARQL查询。RDF-3x和Hexastore为主谓宾分别创建索引。gStore和AmbER在RDF图中利用子图匹配应答SPARQL，和VF2应答SPARQL查询类似。 关键词查询。 将关键词转换为SPARQL，然后利用SPARQL查询引擎。 找到包含所有关键词的子结构，如tree，clique，或其他。 挖掘常用pattern建立索引。gIndex，gSpan，GADDI。 keyword和SPARQL的混合查询。Elbassuoni假定每个三元组都有相联系的文字段落，利用关键词条件扩展SPARQL中的三元组。$CE^2$假定每个资源都有联系的文档，他用关键词条件扩展SPARQL中的变量。但本方法适用范围最广。 其他。[38]中作者为结构化查询中掺杂关键词查询定义了新的查询语言。Bhagdev和Bikakis尝试使用语义进行关键词查询，邹蕾将自然语言转换成SPARQL查询（读过）。9. ConclusionsIn this paper, we proposed a new kind of query (the SK query) that integrates SPARQL and keywords. To handle this kindof query, we first introduced a basic method based on backward search. However, this basic solution faces several performance is- sues. Hence, we built a structural index and a distance-based in- dex. Our structural index is based on frequent star patterns in the RDF data, and our distance-based index is based on the shortest path trees of selected pivots in the RDF graph. Using the indices, we propose an advanced strategy to deal with SK queries. Finally, using three real RDF datasets, we demonstrated that our method can outperform the baseline both with respect to effectiveness and efficiency.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>2017年9月</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Keyword Search in Graphs: Finding r-cliques》——读书笔记]]></title>
    <url>%2F2017%2F08%2F24%2F%E3%80%8AKeyword-Search-in-Graphs-Finding-r-cliques%E3%80%8B%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[冷静一下重新去看这个论文，现在还不是提速的时候。慢慢来这个文章看样子是改了好多其他人的算法，我觉得可以好好看看他是怎么改别人的，学习一波。。。。然后改了他的 ABSTRCT图上的关键词搜索是找到一个包含所有关键词的子结构，之前的工作都是找最小连通图（connected minimal trees），而现在发现找子图比找子树对用户来说更有用。子树中的关键词节点（content nodes）彼此间可能并不紧连，另外，在找子树时会遍历整个图而不仅是关键词节点。An r-clique is a group of content nodes that cover all the input keywords and the distance between each two nodes is less than or equal to r. 1. INTRODUCTION keyword search in databases.是联通子树（这是一本书，我没看。。。）。李国良的文章Ease: Efficient and adaptive keyword search on unstructured, semi-structured and structured data 的答案是半径不大于r的斯坦纳树。 关键词搜索在结构化数据上$\uparrow$。结构化数据$\rightarrow$图。以前基于树或图的方法存在的问题： 一些关键词节点（content nodes）相距太远。 对所有的节点遍历，时间和空间复杂度都很高。 使用r-cliques的优点： 所有关键词节点相距很近。 无需遍历所有节点。 举例说明：关键词：James, John, Jack。 r=10。图2(a)中的答案比图2(c)中的更加合理，因为三个点在同一个组织。另外三边之和也是图2(a)更小。但是对于用文献13生成的两者的斯坦纳树，结果相反。 本文贡献： 提出一个新的图关键词搜索的模型。 证明找到有最小权重的r-clique是NP-hard问题。 基于Branch和Bound的算法找到所有的r-cliques。 提出一个找2-approximation r-cliques的近似算法，能在多项式时间内以升序找到所有的r-clique。 为了找到某个r-clique中的节点的关系，提出在图中找连接r-clique中节点的斯坦纳树。 2. RELATED WORD基于树的算法 Keyword searching and browsing in databases using banks.——基于后向搜索产生斯坦纳树的算法。 Keyword searching and browsing in databases using banks.——动归寻找斯坦纳树的算法。 Keyword proximity search in complex data graphs.——多项式内寻找生成斯坦纳树。 Bidirectional expansion for keyword search on graph databases.——生成不同根的子树。 Blinks: ranked keyword searches on graphs.——使用更有效地索引。 基于图的算法 Ease: Efficient and adaptive keyword search on unstructured, semi-structured and structured data.——寻找包含所有关键词的r-radius的斯坦纳图。但如果一些高排名的子图被包含在其他大图中，该方法会遗漏。另外还产生重复和冗余的结果。 Querying communities in relational databases.——寻找多中心的子图（communities），每个content节点到每个中心点距离存在小于$R_max$的路径。排序方式：中心点到所有content节点的路径长度的最小值。 本方法解决了community方法的三个问题： r-cliques保证所有content nodes紧邻。 只搜索content nodes提高运行效率。 通过生成斯坦纳树减少不相干的节点。 本方法来源：Minimum-diameter covering problems.——与文中Multiple Choice Cover问题很相关。Finding a team of experts in social networks.——上文算法中的一个应用。两者都是找单个答案。本文找top-k，用所有点对间的距离作为得分函数。本方法和Finding a team of experts in social networks.中的图模式匹配问题相似。 3. PROBELM STATEMENT本文中只考虑有权无向图，但很容易应用到有向图中（在判断r的时候再当成无向图）。 Problem 1. Given a distance threshold r, a graph G and a set of input keywords, find an r-clique in G whose weight is minimum.Theorem 1. Problem1是NP-hard的。 4. BRANCH AND BOUND ALGORITHM分支定界法：基于系统的枚举并使用距离限制r，并不排序。先构造关键词的倒排列表，然后把第一个关键词的所有节点加入rList，然后依次加入其它关键词的候选节点，检查是否能和rList中保存的结构成为r-clique，如果可以则加入。为加速算法，预存了所有点对之间的距离。算法复杂度为$O(l^2|C_{max}|^{l+1})$。l代表关键词数量，$C_{max}$为$C_i$的最大size。 5. POLYNOMIAL DELAY ALGORITHM分支定界法慢而且不排序。 5.1 Main Procedure本算法改编自 E. Lawler. A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem（这个文章是J. Yen. Finding the k shortest loopless paths in anetwork.的后延）.Lawler的算法：搜索空间先被划分为互补相交的子空间，然后子空间的答案被生成当前全局最优的答案。然后得到最优答案的子空间接着被划分。。。有两个关键问题：怎么划分子空间（和13相似）&amp;如何在子空间中找最佳答案。如果一个节点只包含一个关键词，则生成无重复的答案。（为什么？）如果包含多个关键词，则需要进行剪枝。 如果最佳答案是${v_1,v_2,v_3,v_4}$，则如table1去划分子空间，然后分别在四个子空间找到最优答案，然后对包含四个中最优答案的子空间再次进行划分，如此循环~ 5.2 Finding Best Answer from a Search Space 对搜索空间中的每个点都计算其到其他关键字节点集合的最短距离$d(s^i_j,k)$，以及集合中对应的节点$n(s^i_j,k)$（3-6初始化，7-16计算），然后对每个点判断是否符合r-clique的定义，如果符合，则加入到候选集合中，最后取出top-k。算法复杂度$O(l^2|S_{max}|^2)$ Exact Top-k Nearest Keyword Search in Large NetworksThere are other keyword search problems that are of some different characteristics. The general idea of keyword search is to find a subgraph in a given graph that contains the query keywords.The subgraph can be of the form of a tree in some cases. BANKS in [5] converts a relational database into a graph and answers to keyword queries are directed subtrees in the graph. Given a directed graph, the keyword search in [16] returns top ranked subtrees in the graph that cover the query keywords. Blinks [18] also considers directed graph and given a keyword query, an answer is a subtree in the graph that covers the keywords and the root of the subtree can reach all the keywords. Top-k results are top k subtrees with different roots. The graph type of r-clique is introduced in [21] as the form of expected answers. An r-clique is a set of vertices in the given graph which covers the given query keywords and the distance between any pair of the vertices in this set is no longer than r. Both exact and approximate algorithms have been proposed in [21]. Querying the neighbors of a vertex in a compressed social network is considered in [24] Efficient processing of keyword queries over graph databases for finding effective answers一篇较新的关于树形的文章。 the minimal Steiner tree semantics.将答案树的权重定义为边的和，所以问题就转化成optimal group Steiner tree problem.有些人用启发式的规则得到l倍近似的结果（l是关键词数目）；有人用dp…但这些方法并不能有效的在大图上得出top-k Steiner tree-stuctured answers. distinct root semantics.答案树的权重为根节点到关键词节点的最短路径之和，每个根节点只有权重最小的答案树被作为候选。因此，对于n个点的图，至多有n个答案，所以比斯坦纳树更高效（这里没懂）。 之前的工作限制着包含一个关键词的节点个数有且只有一个，本文中答案树包含一个关键词的节点可能有多个。 Survey on Keyword Search over XML Documents 该文第3章提出对于XML关键词搜索基于图的方法。 Subtree based Semantics for Directed Graphs. the minimal Steiner tree semantics. Keyword proximity search in complex data graphs. the distinct root semantics. BLINKS: ranked keyword searches on graphs Subgraph based Semantics for Undirected Graphs. (方法的优劣暂且不论，其应用在无向图，但RDF是有向图) r-radius semantics. EASE: Efficient and adaptive keyword search on unstructured, semi-structured and structured data.（2008） r-clique semantics. Keyword search in graphs: finding r-cliques（2011） minimum cost connected tree. Finding top-k min-cost connected trees in database.（2007） Bi-directed Tree based Semantics for Directed GraphsBANKS和Bidirectional expansion for keyword search on graph databases（2005）返回有前向边或后向边的子树。Finding and approximating top-k answers in keyword proximity search返回混合了前向边和后向边的子树。（2006）]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Keyword</tag>
        <tag>2017年9月</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Natural Language Question Answering over RDF —— A Graph Data Driven Approach》——论文笔记]]></title>
    <url>%2F2017%2F08%2F24%2F%E3%80%8ANatural-Language-Question-Answering-over-RDF-%E2%80%94%E2%80%94-A-Graph-Data-Driven-Approach%E3%80%8B%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ABSTRACTRDF Q/A允许用户对RDF知识库用自然语言提问。为回答该提问，需要两步：理解问题和执行查询。现有工作大都集中在解决自然语言的歧义，通常做法是the joint disambiguation，使搜索空间指数增长。本文从图数据驱动的角度解决该问题，提出使用语义查询图为自然语言的查询意图建模，将问题归约成子图匹配问题。更重要的是，我们通过查询的匹配情况，解决自然语言问题的多义性。实验结果验证算法。 1. INTRODUCTION背景：用户需从知识库中获取知识，RDF格式成为标准，SPARQL查询对用户不友好，需要RDF Q/A系统。 1.1 Motivation 现在RDF Q/A系统主要分两个阶段：question understanding和query evaluation。第一阶段把自然语言问题N转化成SPARQLs，这也是目前大部分工作的研究重点。第二阶段执行第一阶段得到的SPARQls。如图1所示，(a)是RDF数据集，(b)是目前解决方法的两个步骤，可以看到由于多义性，有些短语对应多个实体。如果同时考虑这些短语则增加了响应时间。本文并不在第一阶段解决多义性问题，而是放到第二阶段，能够避免问题理解阶段的高昂的消歧处理，从而加速整个系统。本方法中最关键的问题在于如何定义RDF图G中的子图和自然语言问题N的匹配，以及如何找到匹配。 1.2 Our Approach虽然本方法仍有“question understanding”和“query evaluation”，但并不与现在的SPARQL generation-and-evaluation相同，本方法是graph data-driven，其大概的框架如图1(c)。在question understanding阶段，我们把问题iN翻译为semantic query graph $Q^{S}$。该步允许多义。在query evaluation阶段，我们在图G上找$Q^{S}$的匹配的子图。我们基于语义相似性定义了匹配的分数。将消歧放在query evaluation阶段不仅提高了精度也加速了整个查询应答时间。贡献： 为问题提出了系统的框架。并从graph data-driven的角度，将消歧放在了query evaluation阶段。 离线处理：提出图挖掘算法，将短语匹配到top-k个可能的谓词，形成paraphrase dictionary D。 在线处理：两个阶段，首先将问题N转换为semantic query graph $Q^{S}$，然后把RDF Q/A归约成$Q^{S}$在图G上的子图匹配问题。在找到匹配时解决了多义问题，如果没有匹配发现则消歧的花费就被节省了。 实验 2. FRAMEWORK 本问题有两个关键的挑战，一是如何以结构化的方式表示问题N中的查询意图。二是如何处理问题N中的短语的多义性。为解决第一个挑战，我们从N中抽出semantic relations，并基于此建立了semantic query graph $Q^{S}$为问题N中的问题意图建模。 Definition 1. (Semantic Relation). A semantic relation is a triple , where rel is a relation phrase in the paraphrase dictionary D, arg1 and arg2 are the two argument phrases.Definition 2. (Semantic Query Graph) A semantic query graph is denoted as $Q^S$, in which each vertex $v_i$ is associated with an argument and each edge $\bar{v_iv_j}$ is associated with a relation phrase, $1\le i,j\le |V(Q^{S})|$. 针对第二个挑战，我们提出了数据驱动的方法：对于N中一个短语到实体的映射，如果能找到包含该实体的子图且匹配N中的查询意图，那该映射是正确的；否则是错误的。 2.1 Offline建立了paraphrase dictionary D——记录语义相等的关系短语和谓词。一些现有的系统如Patty和ReVerb还未每个关系短语提供了其支持的实体对，如表2。方法思路：对每个关系短语$rel_i$，$Sup(rel_i)$表示一系列该谓词支持的实体对。我们假设这些实体对也出现在RDF图中。频繁出现的谓词连接$Sup(rel_i)$中的实体对和关系短语$rel_i$等价。基于该想法我们提出一个找语义相等的关系短语和谓词的图挖掘算法。 2.2 Online1) Question Understanding. 目的在于为问题N构建一个语义查询图$Q^{S}$。首先用Stanford Parser得到N的依赖树Y，然后基于paraphrase dictionary D抽取Y中的语义关系。基本的思路是找到一个Y的包含rel的所有词的最小子树。该子树被称为Y中的rel的一个嵌入，并且基于一些语言规则我们得到有联系的两个参数，形成$&lt; rel,arg1,arg2 &gt;$，最后连接这些关系得到查询图$Q^{S}$。2) Query Evaluation. 找到与$Q^{S}$匹配的子图。匹配按照子图同态定义。 首先，$Q^{S}$的点，被映射到RDF图中的一些实体或类，并赋予一个置信度，保存在有序列表$C_{v_i}$。关系短语$rel_{\overline{v_iv_j}}$被映射到候选谓词的列表$C_{\overline{v_iv_j}}$中。列表以置信度排序。本步中并没有解决多义问题。其次， Definition 3.(Match) Consider a semantic query graph $Q^{S}$ with n vertices $\{v_1,…,v_n\}$. Each vertex $v_i$ has a condidate list $C_{v_i},i=1,…,n.$ Each edge $\overline{v_iv_j}$ also has a candidate list of $C_{\overline{v_iv_j}}, where $1\le i\ne j\le n.$ A subgraph M containing n vertices $\{u_1,…,u_n\}$ in RDF graph G is a match of $Q^{S}$ if and only if the following conditions hold: if $v_i$ is mapping to an entity $u_i$, i=1,…,n, $u_i$ must be in list $C_{v_i}$; if $v_i$ is mapping to a class $c_i$, i=1,…,n, $u_i$ is an entity whose type is $c_i$ (i.e., there is a triple &lt;$u_i$ rdf:type $c_i$&gt; in RDF graph) and $c_i$ must be in $C_{v_i}$; $\forall \overline{v_iv_j}\in Q^S; \, \overrightarrow{u_iu_j}\in G \lor \overrightarrow{u_ju_i}\in G$. Furthermore, the predicate $P_{ij}$ associated with $\overrightarrow{u_iu_j}$ (or $\overrightarrow{u_ju_i}$ is in $C_{\overline{v_iv_j}},\, 1\le i,j\le n$. 每个和$Q^S$匹配的子图都有一个得分，由边和点的概率决定。我们的目标是找到top-k个匹配的子图，在4.2.2节中解决。 3. OFFLINE 语义关系抽取依赖于词典D，图3是词典的一个示例。本文并不讨论如何抽取短语及其对应的实体对，假设已经给定。在offline中的任务是找到语义相等的关系短语和RDF中的相应谓词，即构建如图3的词典D。假设已有词典$T=\{rel_1,…,rel_n\}$，每个$rel_i$都是一个关系短语，并有一个出现在RDF图中的实体对集合，即$Sup(rel_i)=\{(v^1_i,V^{‘1}_i),…,(v^m_i,V^{‘m}_i),\}$。对于每个$rel_i$目标是找到RDF图中的top-k个语义相等的谓词（路径）。方法：给定一个关系短语$rel_i$及$Sup(rel_i)=\{(v^1_i,V^{‘1}_i),…,(v^m_i,V^{‘m}_i),\}$，对$(v^j_i,V^{‘j}_i),j = 1,…,m$，我们在RDF图中找到两点间的简单路径：$Path(v^j_i,V^{‘j}_i)$，如图4。则$PS(rel_i)=\bigcup_{j=1,…,m}Path(v^j_i,V^{‘j}_i)$. 为了效率，我们设定了路径的阈值，然后使用双向的BFS搜索找到$Path(v^j_i,V^{‘j}_i)$。但是这样的方法会带来噪音，解决方法：采用了tf-idf度量。 Definition 4. Given a predicate path L, the tf-value of L in $PS(rel_i)$ is defined as follows: tf(L,PS(rel_i))=|\{Path(v^j_i,V^{'j}_i)| L\in Path(v^j_i,V^{'j}_i)\}|The idf-value of L over the whole relation phrase dictionary $T=\{rel_1,…,rel_n\}$ is defined as follows: idf(L,T)=log \frac{|T|}{|\{rel_i\in T|L \in PS(rel_i)\}|+1}The tf-idf value of L is defined as follows: tf-idf(L,PS(rel_i),T)=tf(L,PS(rel_i))\times idf(L,T) 关系短语和谓词（路径）的置信度定义为： \delta (rel,L)=tf-idf(L,PS(rel_i),T) \tag{1} 算法1展示了为每个关系短语找top-k谓词路径的细节。注意tf-idf is a probability value to evaluate the mapping (from relation phrase to predicate/predicate paths) confidence.维护D只需要为新引入的谓词重新挖掘映射，或删除被移除数据集的谓词的映射。 Theorem 1. The time complexity of Algorithm 1 is $O(|T|\times |V|^2\times d^2)$, where |T| is the number of relation phrases in T, |V| is the number of vertices in RDF graph G, and d is the maximal vertex degree. 4. ONLINE4.1 Question Understanding本节讨论如何识别问题N中的语义关系，并基于关系建立语义查询图$Q^S$代表N中的查询意图。为抽取句子关系短语，建立依赖树。图5展示了问题N的依赖树表示为Y。 Definition 5. Let us consider a dependency tree Y of a natural language question N and a relation phrase rel. We say that rel occurs in Y if and only if there exists a connected subtree y (of Y) satisfying the following conditions: Each node in y contains one word in rel and y includes all words in rel. We cannot find a subtree $y^{‘}$ of Y, where $y^{‘}$ also satisfies the first condition and y is a subtree of $y^{‘}$.In this case, y is an embedding of relation phrase rel in Y. 给定问题N的依赖树Y和关系短语词典$T=\{rel_1,…,rel_n\}$，我们需要T中那个关系短语在Y中出现。 4.1.1 Finding Relation Phrase Embeddings Theorem 2. The time complexity of Algorithm 2 is $O(|Y|^2)$. 4.1.2 Finding Associated Arguments通常参数的识别依赖subject-relations、object-like relations，如下： subject-like relations: sbj, nsubj, nsubjpass, csubj, csubj-pass, xsubj, poss; object-like relations: obj,pobj, dobj, iobj 假设关于短语rel的嵌入子树为y。通过检查y中的每个节点w及其子节点是否出现以上的subject-like（object-like）关系，出现则把子节点加入到arg1（arg2）。如果arg1/arg2仍是空，我们有以下启发式规则： Rule 1: Extend the embedding t with some light words, such as prepositions, auxiliaries. Recognize subject/object-like relations for the newly added tree node. Rule 2: If the root node of t has subject/object-like relations with its parent node in Y, add the root node to arg1. Rule 3: if the parent of the root node of t has subject-like relations with its child, add the child to arg1. Rule 4: If one of arg1/arg2 is empty, add the nearest wh-word (such as what, who and which) or the first noun phrase in t to arg1/arg2. 如果arg1/arg2仍然是空，则放弃关系短语rel。 4.1.3 Building Semantic Query Graph把语义关系表示为边，如果两个关系的参数相同，则边相连。 4.2 Query Evaluation4.2.1 Phrases Mapping讨论如何将关系短语和参数映射到候选的谓词（路径）和实体。Mapping edges of $Q^S$. $Q^S$中边$\overline{v_iv_j}$对应关系短语$rel_{\overline{v_iv_j}}$。按照paraphrase dictionary D，$rel_{\overline{v_iv_j}}$映射到列表$C_{\overline{v_iv_j}}$，列表中是谓词P或谓词路径L。$\delta(rel,L)$置信度。Mapping Vertices of $Q^S$. $Q^S$中点v对应参数arg。如果arg是wh-word，则它可映射到RDF中所有实体和类，否则返回一个对应的实体或类的列表。本文使用现成的工具DBpedia Lookup。$\delta(arg,c)$置信度。Graph Data-driven Disambiguation. graph data-driven solution. 4.2.2 Finding top-k Subgraph Matches Definition 6. Given a semantic query graph $Q^S$ with n vertices $\{v_1,…,v_n)\}$, a subgraph M containing n vertices $\{u_1,…,u_n\}$ in RDF graph G is a match of $Q^S$. The match score is defined as follows: Score(M)= log( \prod_{v_i\in V(Q^S)} \delta (arg_i,u_i) \times \prod_{\overline{v_iv_j}\in E(Q^S)} \delta (rel_{\overline{v_iv_j}},P_{ij}) )\\ =\sum_{v_i\in V(Q^S)} log( \delta (arg_i,u_i))+\sum_{\overline{v_iv_j}\in E(Q^S)} log( \delta (rel_{\overline{v_iv_j}},P_{ij}) ) \tag{2}where $arg_i$ is the argument of vertex $v_i$, and $u_i$ is an entity or a class in RDF graph G, and $rel_{\overline{v_iv_j}}$ is the relation phrase of edge $\overline{v_iv_j}$ and $P_{ij}$ is a predicate of edge $\overrightarrow{u_iu_j}$ or $\overrightarrow{u_ju_i}$ 给定语义查询图$Q^S$，我们目标是找到$Q^S$的所有匹配中分数top-k的。这是个NP-hard问题。 Lemma 1. Finding Top-1 subgraph match of $Q^S$ over RDF graph G is an NP-hard problem.Lemma 2. Finding Top-k subgraph match of $Q^S$ over RDF graph G is at least as hard as finding Top-1 subgraph match of $Q^S$ over G.Theorem 3. Finding Top-k subgraph match of $Q^S$ over RDF graph G is an NP-hard problem. 因为他是NP-hard问题，所以我们设计启发式规则减少搜索空间。第一个利用neighborhood-based pruning减少$C_{v_i}$和$C_{\overline{v_iv_j}}$。第二个是基于top-k匹配的分阈值及早停止搜索。 5. TIME COMPLEXITY ANLYSIS 6. EXPERIMENTS]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>2017年8月</tag>
        <tag>RDF</tag>
        <tag>QA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Scalable Keyword Search on Large RDF Data》——论文笔记]]></title>
    <url>%2F2017%2F08%2F20%2F%E3%80%8AScalable-Keyword-Search-on-Large-RDF-Data%E3%80%8B%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Abstract目前关键词搜索的两种方法：一、依赖建立距离矩阵来剪枝搜索空间，二、为RDF图建立摘要。本文指出现有技术面对真实数据集时的不足，并且提出一个新的摘要算法，能够更有效的剪枝并得到正确的答案。 1 Introduction 动机：RDF数据急速增长。关键词搜索对大规模数据十分有用，目前对于RDF数据的解决方法的局限： 返回不正确的答案。 难以处理大规模RDF数据。 目标：设计一个能处理大规模RDF数据集的scalable and exact soluton。 贡献： identify and address limitations in the existing methods for keyword search in RDF data. 并基于后向搜索提出一个正确的baseline解法。 develop efficient algorithms to summarize the structure of RDF data, based on the types in RDF graohs. 和之前的方法相比more scalable剪枝也更有意义，并且得到的摘要是轻量级的而且可更新。 experiments on both benchmark and large real RDF datasets. 2 Preliminaries将RDF数据集看做RDF图G=(V,E)其中 $V=\{V_E,V_T,V_W\}$ $V_E$: the set of entity vertices. $V_T$: the set of type vertices. $V_W$: the set of keyword vertices. $E=\{E_R,E_A,E_T\}$ $E_R$: the set of entity-entity edges. $E_A$: the set of entity-keyword edges. $E_T$: the set of entity-type edges. 图1中主要结构被entity-entity edge即$E_R$捕获，因此将entity vertex和关于他的type vertex和keyword vertex看做一个点，我们得到RDF图G的压缩视图，表示为$G_c=\{V^{‘}_E,E_R\}$。其中$|V^{‘}_E| \equiv |V_E|$，$v^{‘}\in V^{‘}_E$包含$v\in V_E$和与之联系的类型和关键词节点。 2.1 Problem statement关键词查询问题即RDF图中寻找包含所有关键词的子图。为便于表示，假设每个节点只包含一个关键词。（但对于包含多个和不包含的也能处理）对在$G=\{V,E\}$上的查询$q=\{w_1,w_2,…,w_m\}$，点集$\{r,v_1,…,v_m\}$在以下条件成立时被称为 qualified candidate: root answer node $r\in V$能够到达任一点$v_i\in V,\, i\in [1,m]$ $w(v_i)=w_i$ A(q): the answer for qC(q): the set of all qualified candidates in G with respect to q. A(q)= arg min_{g\in C(q)}s(g), \, and \, s(g)=\sum_{r,v_i\in g, i=1..m}d(r,v_i)\tag{1}其中$d(r,v_i)$是从r到$v_i$的距离（不考虑边方向）。 该定义还有一个top-k版本，其中对每个$g\in C(q)$的得分s(g)进行升序排列，得到前k个答案。 3 Related work许多技术假设图能够在内存中处理，如14 17为所有的点对保存了距离矩阵。另外这些工作不考虑如何处理更新。本文中我们将后向搜索应用于大RDF图并经过严密的证明，不依赖于距离矩阵。关于摘要大图来支持关键词搜索的技术来自于[9]，作者假设块之间的边是有权重的，块被当做supernode，块之间的边被当做superedges，它们组成摘要图。然后循环此过程。该方法针对通用图，并不能拓展到RDF图中。[23]研究了RDF图上的关键词搜索，和本文一样，其调整了[14]中的问题定义。该方法从RDF数据集中摘要出schema，在schema中应用后巷搜索得到最有可能的关系，然后将关系转化为SPARQL中的pattern进行检索。[23]中摘要算法的局限：将同一类型的所有实体归结到一个点，丢失了太多信息，以至于产生错误结果。另外该方法不支持更新。[18]中通过对异质关系编码为图，支持结构化，半结构化和非结构化的关键词查询。同样的，他也需要距离矩阵。11 12研究了排序函数。我们则是调整了RDF[23]和通用图[14]中的排序函数。 4 The Baseline Methodbaseline基于“backward search”的启发式。“backward search”：图中对应关键词查询的所有节点同时开始，迭代地想邻居节点拓展，直到候选答案生成。termination condition用来判断搜索过程是否完成。[23]中的termination condition是当m个节点第一次遇到节点r时返回答案并停止搜索，但该方法并不正确。Counter example. 对于图a来说，第二轮迭代得到$g=\{r=v_4,v_1,v_2,v_6,v_7\}\, \, s(g)=8$，但第四轮迭代中$g^{‘}=\{r=v_3,v_1,v_2,v_6,v_7\}\, \, s(g^{‘})=6$。The correct termination. 如算法1.Data structure. $q=\{w_1,…w_m\}$: query G=\{V,E\}: a (condensed) RDF graph $W_i$: vertices in V containing the keyword $w_i$ $\{a_1,…a_m\}$: m empty priority queues, one for each query keyword. M: 集合中每个元素对应目前探索到的独一无二的node，记录他们能够到达那个关键词以及距离。对于fig5(a)，$M[v_3]=\{(v_1,1),(v_2,1),nil,(v_7,1)\}$The algorithm. (图片中是不是line10,11写错了？）第一轮迭代算是初始化$a_i$和M。$W_i$中每个v和其邻居u放入$a_i$中，并新建$M[u]$或更新$M[u]$的相应值。第二轮迭代首先pop$a_i$的堆顶值，然后添加 $(v,p=\{v,…,u\},d(p))\}$中u的每个邻居$u^{‘}$。将 $(v,p=\{v,…,u^{‘}\},d(p)+1)\}$压入$a_i$然后更新$M[u^{‘}]$。如果$M[u]$没有nil，则该条被标记为候选答案，u为候选根节点。将M[u]中的最短路径表示为g，我们有： Lemma 1 $g=\{r=u,v_{l_1},…,v_{l_m}\}$ is a condidate answer with $s(g)=\Sigma^m_{i=1}d(u,v_{l_i})$. 两种情况：(i) an unseen vertex, i.e., $v\notin M$, will become the answer root(Lemma 2); (ii) a seen but not fully expanded vertex $v\in M$ will become the answer root(Lemma 3).$V_t$: the set of vertices that are not fully explored.$(v_1,p_1,d(p_1)),…(v_m,p_m,d(p_m))$: the top entries from $a_1…a_m$. Lemma 2 Denote the best possible candidate answer as $g_1$, and a vertex $v\notin M$ as the answer root of $g_1$. Then it must have $s(g_1)&gt;\Sigma^m_{i=1}d(p_i)$.Lemma 3 Suppose the best possible candidate answer using such an $v(v\in M\, and \, v\in V_t)$ as the answer root is $g_2$ then s(g_2)>\sum^m_{i=1}f(v_{b_i})d_i + (1-f(v_{b_i}))d(p_i) \tag{2}where $f(v_{b_i})=1$ if $M[v][b_i]\neq nil$, and $f(v_{b_i})=0$ otherwise. The termination condition. 对于情况(i)，我们简单的让$s(g_1)=\Sigma ^m_{i=1}d(p_i)$;对于情况(ii)，we find a vertex with the smallest possible $s(g_2)$ value w.r.t. the RHS of (2), and simply denote its best possible score as $s(g_2)$.Denote the kth smallest candidate answer identified in the algorithm as g, our search can safely terminate when $s(g)\le min(s(g_1),s(g_2))=s(g_2)$. Theorem 1 The Backward method finds the top-k answers A(q,k) for any top-k keyword query q on RDF graph. 5 Type-Based SummarizationBackward方法对大的RDF图不适用，因为Backward为完成搜索，会构建无数的搜索路径。为了减少Backward算法的输入规模，只在有希望的子图上应用。我们提出了一个type-based摘要方法，即先在摘要图上进行关键词搜索，剪枝掉大部分无用的结果，然后再应用Backward。The intuition. 首先对RDF图分区，被查询的关键词首先由分区连接。挑战在于如何对不会产生top-k跨区答案进行剪枝。要做到这个我们需要对跨越分区的后向搜索的路径距离进行校正。但维护所有路径的距离成本太高，因此我们提取一个可更新的摘要图，使得任何后向搜索可以被有效地估计。The key observation: 紧邻的相同类型邻居节点一般共享相似的结构——和其他类型的节点的连接，如fig6。我们基于以上观察构建一个typed-based summary。 5.1 Outline and preliminaries首先将RDF图划分为多个小的区，然后定义摘要了分区的type-baseed structures。摘要保存所有分区的不同结构。通常关键词搜索在两方面受益于摘要： we can obtain the upper and lower bounds for the distance traversed in any backward expansion without constructing the actual path (Section 6). we can efficiently retrieve every partition from the data by collaboratively using SPARQL query and any RDF store without explicity storing the partition (Section 15). 两个定义：Homomorphism across partitions. 如图6(a)所示，邻近的类型节点是生成induced partitions的好的源头。图6(a)是图6(b)的子集。We consider discovering such embeddings between the induced partitions, so that one template can be reused to bookkeep multiple structures. Definition 1 A graph homomorphism f from a graph $G=\{V,E\}$ to a graph $G^{‘}=\{V^{‘},E^{‘}\}$, writtern as $f: G\rightarrow G^{‘}$, is a mapping function $f: V\rightarrow V^{‘}$ such that(i) f(x)=x indicates that x and f(x) have the same type; (ii) $(u,v)\in E$ implies $(f(u),f(v)) \in E^{‘}$ and they have the same label. When such an f exists, we say G is homomorphic to $G^{‘}$. Cores for indeividual partitions. A core is a graph that is only homomorphic to itself, but not to any one of its proper subgraphs.Definition 2 A core c of a graph G is a graph with the following properties: there exists a homomorphism from c to G; there exists a homomorphism from G to c; and c is minimal with these properties. 5.2 Partition摘要过程开始于将数据分成较小的，语义相似的，边不相交的子图。鉴于我们观察到相同类型的节点共享相似的类型邻居，我们基于类型用环绕相同类型节点的子图对G划分。算法使用RDF的condensed视图。$\{T_1,…,T_n\}$: n distinct number of types.$V_i$vertices whose type is $T_i$.h(v,$\alpha$)(the $\alpha$-neighborhood of v): the subgraph from G obtained by expanding v with $\alpha$ hops. 拓展时的边不在P中，且是有向图，所以h(v,$\alpha$)是v $\alpha$跳邻居节点的子集。P：初始化为空，然后每个h(v,$\alpha$)都是一个新的划分。 Lemma 4 Partitions in P are edge disjoint and the union of all partitions in P cover the entire graph G. 我们遍历类型的顺序不同可能会影响分区P的最终结果。但无论怎样，同种类型的节点总是基于其$\alpha$-neighborhoods生成一系列划分。如图8。 5.3 Summarization摘要算法从P的分区集合中识别出一系列templates。这些templates是partitions 的摘要。另外摘要算法保证P中的每个分区都与某个templates同态。该特性是的查询优化器： 不用频繁访问RDF数据的前提下有效地在后向拓展时估计路径长度。 通过查询RDF数据来有效地重构感兴趣的分区，而不显式地存储和索引分区。 给定一个分区P，算法3检索出所有的不同的结构并将其保存在S中。Improving efficiency and reducing |S|.算法3的两个问题：(1)在3,5,7行需要判断同态，这是NP-hard问题。(2) 尽量减少|S|的大小，以便能够放入内存中处理。对$h(v,\alpha)$的边建立一个covering tree，即$h_t(v,\alpha)$。并用$h_t(v,\alpha)$代替$h(v,\alpha)$。Example 2. 图9中，$h(v_1,2)$$中$v_4$节点在不同边中被访问了三次，所以有三个拷贝。该方法的优点： 降低S中不同结构的数量，如图9所示，两个在数据层面不同的结构，在类型层面共享一个结构。 对于通用图来说，检测子图同态十分耗时。但能在多项式时间内检测类型层面的结构。 5.4 Auxiliary indexing structures为了帮助关键词搜索，我们维护了三个辅助列表。a portal node：node that isincluded in more than one partitions. portal index for each partition $h(v,\alpha)$, 我们赋予其唯一id并和portal列表联系。$\sigma (v_i)$: 表示$h_t(v,\alpha)$中的所有$v_i$。$\Sigma=\{\sigma (v_1),\sigma(v_2),…\}$：表示一个分区中所有的一对多的映射。如图9中，$h(v_1,2)$$\Sigma \leftarrow \{\sigma (v_4)=\{T_4\}\}$. partition index: to map the partition root v of $h(v,\alpha)$ to its $\Sigma$.summary index: 将partitions中的节点映射到S中的摘要节点。sid: S中的每个摘要的id，nid: S中每个节点的id。为了获得每个$h_t(v,\alpha)$到S中的summary的映射，需要建立日志保存建立S时的发现的所有同态。等S建立完成后我们遍历日志找到所有从数据到summary的映射。过程如图10. 6 Keyword search with summary搜索算法，摘要层和数据层的两级后向搜索。只有摘要层中被识别的connected partitions包含所有关键词，并且其分数在top-k，才会进入数据层执行后向搜索。路径长度计算是后向搜索和剪枝的核心，但摘要层并不能拿到准确的路径长度，因此首先展示如何估计路径长度，然后介绍算法。 6.1 Bound the shortest path length通过summary index，分区根节点v到分区内任一节点u的最短距离可计算，所以由三角形不等式得：$|d(v,v_1)-d(v,v_2)|\le d(v_1)-d(v_2)\le |d(v,v_1)+d(v,v_2)|$。另外，另一个下界可用根节点v所在分区的同态的摘要图得到，即Lemma 5： Lemma 5 Given two graphs g and h, if $f:g\rightarrow h$, then $\forall v_1,v_2\in g$ and their homomorphic mappings $f(v_1),f(v_2)\in h,\, d(v_1,v_2)\le d(f(v_1),f(v_2))$. 如图11，从h到s没有直接的同态，因此不能直接应用Lemma 5。定义映射函数Join。输入：图g，$\{V^{‘}_{t_1},V^{‘}_{t_2},…\}$。输出：新图$g^{‘}=Join(g(V,E),\{V^{‘}_{t_1},V^{‘}_{t_2},…\})$。其中$V^{‘}_{t_i}$中的点都属于类型$t_i$。函数流程： 用g初始化$g^{‘}$； 将$g^{‘}$中的$V^{‘}_{t_i}$合并至类型为$t_i$的点$v^{‘}_{i}$，点集$V^{‘}_{t_i}$中的所有边也赋于$v^{‘}_{i}$； 对所有类型重复步骤2。 Example 3. 以图9为例，$Join(h_t(v_1,2),\{\Sigma(T_4)\})$重建了$h(v_1,2)$，因此两者同态。另外，$Join(h_t(v_5,2),\{\Sigma(T_4)\})$没有重建$h(v_5,2)$，但等于$h(v_1,2)$也和$h(v_5,2)$同态。 Lemma 6 For a partition h and its covering tree $h_t$, there is a homomorphism from h to $Join(h_t,\Sigma)$.Lemma 7 For a partition h, its covering tree $h_t$ and its summary s that has $f_2:h_t\rightarrow s$, there is a homomorphism from $Join(h_t,\Sigma)$ to $Join(s,f_2(|Sigma))$. 如图11b，由Lemmas 6,7和同态的可传递性，h is homomorphic to $Join(s,f_2(\Sigma))$。其中$f_2$是summary index的一部分，将数据中的节点映射到摘要中的节点。最后，给定h中任意两点，其最短路径可有Lemmas 5,6,7和最短路径算法在$Join(s,f_2(\Sigma))$中找到最短路的一个下限。实际应用中，我们从summary和三角不等式中找一个更高的下限。 6.2 The algorithm Data structures. $q=\{w_1,…w_m\}$: query G=\{V,E\}: a (condensed) RDF graph $W_i$: vertices in V containing the keyword $w_i$ $\{a_1,…a_m\}$: m empty priority queues, one for each query keyword. M: 集合中每个元素对应目前探索到的独一无二的node，记录他们能够到达那个关键词以及历经的分区。M中每个条目是四元组$(u,S,d_l,d_u)$。u是后向搜索中包含关键$w_i$的第一个节点。S存储搜索过程中路径的一系列partitions及其portal（exit node），因此S是(portal, partition root)的集合。The algorithm. In the first iteration. 对于每个来自$W_i$的点u，我们从summary index对u所述的分区根节点v检索。并将检索结果插入M和$a_i$中。 In the j-th iteration. 从所有的$a_i$中pop出最小的条，即$(v,(u,S,d_l,d_u))$(line 10). v是当前分区的根节点。S中最后的对是$(l,v_l)$：路径在$v_l$点离开根为$v_l$的分区并进入现分区。$\mathcal{L}=\{\mathcal{l}^{‘}_1,\mathcal{l}^{‘}_2,…\}$表示根节点为v的分区的portals。对于每个$l^{‘}$按照6.1节的方法计算$d(l,l^{‘})$或$d(u,l^{‘})$的上下限。然后第14行更新，其中$v_r$是与$l^{‘}$相连的下一个分区的根节点。M只有在两种情况下停止更新：(i)S的路径产生了环。(ii)$d_l+d^{‘}_l$比当前第i个列表中第k大的上限还要大。 如果M[v]的所有条目非空，则以v为根节点的分区是候选答案。将m个关键词对应的列表组合起来就是联通子图。 然后我们拿到所选分区的实际数据（如何拿到在第7节），进行二级搜索。 Termination condition. Lemma 8,9. Lemma 8 Denote an entry in the priority queue as $(v,(u,S,d_l,d_u))$, then for any $v^{‘}$ in the partition rooted at v and the length of any path starting from u and using the portals in S is $d(u,v^{‘}\le d_l$.Lemm 9 Denote the top entry in the priority queue $a_i$ as $(v,(u,S,d_l,d_u))$, then for any explored path p from $w_i$ in the queue $a_i$, the length of p, written as d(p), has $d(p)\le d_l$.Lemma 10 let $g_1$ be a possible unexplored candidate answer rooted at a vertex in a partition h, with $h\in P_t$, s(g_1)>\sum^m_{i=1}{d}^i_l\tag{3}Lemma 11 Denote the bset possible unexplored candidate answer as $g_2$, which is rooted at a vertex in the partition h where $h\in P-P_t$, then s(g_2)>\sum^m_{i=1}f(t_i)\hat{d}^i_l+(1-f(t_i))d^i_l, \tag{4}where $f(t_i)=1\, if \, t_i\neq nil\, otherwise \, f(t_i)=0.$ The termination condition. 在未探索分区最有可能的答案是$s(g_1)$，如(3)式。在所有探索分区的最有可能的答案是$s(g_2)$，如(4)式。将得分升序排名第k的答案表示为g，则算法停止条件是$s(g)\le min (s(g_1),s(g_2))$. Theorem 2 Summ finds the top-k answers A(q,k) for any top-k keyword search query q on an RDF graph. 7 Accessing data and update在对摘要图搜索完成后，我们需要从实体数据中检索出所选择分区，一个常用的方法是将三元组按分区存储并编上分区的id，但这样更新比较麻烦，并需要独立的存储。我们将RDF数据存储在RDF中，并通过构建的SPARQL查询动态的检索出该分区的数据。 Theorem 3 Homomorphism Throrem [1]. Let $q$ and $q^{‘}$ be relational queries over the same data D. Then $q^{‘}(D)\subseteq q(D)$ iff there exists a homomorphism mapping $f: q\rightarrow q^{‘}$. 因为$c\rightarrow h_t \rightarrow h$因此用c作为SPARQL查询的pattern并结合Theorem 3可以抽取h。两个关键问题： 从$h_t$的集合到c通常有多对一的映射，使得若用c为query pattern会导致a low selectivity. 为解决此问题，我们在query pattern中从目标分区到相应变量间绑定了常量。 在构建S的过程中，并不保存每个c，而是当c是s的子树时将c插入到$s\in S$中。为了从s中构建SPARQL，首先找到根节点，然后拓展到叶子。 8 ExperimentsDatasets]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Keyword</tag>
        <tag>2017年8月</tag>
        <tag>RDF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《RDF Keyword-based Query Technology Meets a Real-World Dataset》——论文笔记]]></title>
    <url>%2F2017%2F08%2F15%2Ftest%2F</url>
    <content type="text"><![CDATA[nucleus ABSTRACT本文介绍了一个工业项目，其通过将RDF技术和关键词搜索结合，开发出一种便于利用碳氢化合物对大型数据库进行数据访问的工具。该工具的特色是通过RDF schema和RDF数据，无需用户介入将关键词转化为SPARQL查询。工具还提供了一系列接口，如specify keywords, as well as filters and unit measures, and presents the results with the help of a table and a graph方便用户使用。 1. INTRODUCTION首先分析现有的网络上的关键词搜索技术，总结其成功的原因： 简单易用的用户接口 有效的文档检索机制 符合用户期望的排序算法 对比来看，数据库管理系统提供了复杂的查询语言，一些数据库应用虽然创建了用户接口，让用户填一些空进行查询，隐藏查询语言的复杂性，但并不友好。我们提供关键词搜索接口，通过把关键词转换为查询语言，将用户从精准填空中解放出来。 关于关系型数据库的关键词查询出现了一段时间，现在也出现了关于RDF数据上的关键词查询。RDF不区分数据和元数据，因此关键词可能和类的名字、属性的描述或者数据的值匹配。RDF管理系统有时和提供推理层会以surpass/relational 视图对RDF数据产生推导数据，因此关键词也可能匹配推导数据。 本文贡献： 定义RDF数据上关键词查询的答案。 通过利用RDF的schema和RDF数据集将关键词查询转换为SPARQL查询。 通过自动补全功能和filter、 unit measures的帮助允许用户精确关键字。 进行实验验证了工具的性能。 2. Related WorkKeyword-based query processing. 分为以下几类： schema-based：使用conceptual schema编译查询。 graph-based：直接在图上操作。 parttern-based：从RDF数据中挖掘pattern替代conceptual schema。 fully automatic：在关键词查询时无需依靠用户干预。 BANKS and BLINKS是早期的graph-based工具。schema-based工具基于candidate networks （CNs）探索外键将关键词转化为SQL。例子有：DISCOVER，DBXplorer。 SPARK是早期的pattern-based RDF graph-based tool。[21]提出想法：利用类的层级从原始图中生成summary graphs，[26]负责实现。[24]挖掘tree pattern。[27]提出挖掘等价的structure patterns to summarize 知识图。[7]基于张量计算对RDF关键词查询。 QUICK[25]是一个RDF schema-based tool，需要用户介入。 本文的工具是schema-based并且fully automatic。从早期的graph-based工具中借鉴了生成由RDF schema引发的图的斯坦纳树来减少equijoins的想法。我们引入了新概念nucleus，其包含一个类，一个属性列表，一个属性值列表。nucleus 一定程度上和tuple相似。然后Steiner tree把那些包含关键词的nucleus连接起来。 和QUICK比较相似，但我们的RDF数据有rich schema并且低歧义，所以我们是全自动的转换。 Triplification of the relational database. 因为关系数据库经常是normalized不可直接映射到RDF，我们首先创造了定义了unnormalized关系视图，然后利用R2RML映射。设计良好RDF schema帮助了关键词到SPARQL的转化，首先，RDF数据集具有已知模式的假设不应被视为缺点。实际上，大部分的LOD数据集确实有一个已知的模式（词汇或本体）[17]。此外，在像我们这样的企业环境中，RDF数据集通常是关系数据库的三元组化。第二，即使不能改变（关系或RDF）模式，也可以添加一个在视图的帮助下定义的概念层，这些概念层隐藏规范化，在关系情况下，或设计不当的RDF模式，这两种情况都会导致处理基于关键字的查询时的歧义。 Benchmarks. 他人所用的数据集及查询。 3. BASIC DEFINITIONS3.1 RDF EssentialsIRI(Internationalized Resource Identifier)：表示一个资源。literal：一个基础值，如字符串，数字等blank node：local identifier，可以被新的IRI替代。 本文中IRI代表所有的IRI的集合，L代表所有的literal的集合。 (s,p,o): s-IRI/ a blank node. p-IRI, o-IRI,a blank node or a literal. RDF 三元组和RDF图等价。 RDF Schema 不提供实际的应用程序专用的类和属性，而是提供了描述应用程序专用的类和属性的框架。RDF Schema 中的类与面向对象编程语言中的类非常相似。这就使得资源能够作为类的实例和类的子类来被定义。介绍了RDF Schema 及其一系列属性 An RDF schema is a set S of RDF triples that use the RDF-S vocabulary to declare classes, properties, property domains and ranges, and sub-class and sub-property axioms. A simple RDF schema is a RDF schema that contains only class declarations, object and datatype property declarations and subclass axioms (and no sub-property axioms). 我们引入一个labelled graph, $D_s$ 被称为RDF schema diagram： the nodes of DS are the classes declared in S; there is an edge from class c to class d labelled with subClassOf iff c is declared as a subclass of d in S, and there is an edge from class c to class d labelled with p iff p is declared in S as an object property with domain c and range d. RDF 数据集T follows RDF schema S的条件： $S\subseteq T$ T中的所有类和属性在S中都被定义 T中的三元组除了那些在S中的都满足S中声明的限制。 3.2 Keyword-Based QueriesT: an RDF dataset.$G_T$: $G_T$ is the corresponding RDF graph.S: an RDF schema.K: A keyword-based query—- a set of literals.match $\mathbf L \times \mathbf L \rightarrow [0,1]$:literal之间的相似函数。$\sigma \in (0,1]$: similarity threshold.MM[K,S]:metadata matches between K and metadata descriptions of the classes and properties in S. MM[K,T]=\{ (k,(r,p,v)) \in K\times T/ (r,p,v)\in S \wedge match(k,v)\le \sigma\}VM[K,T]: property value matches between K and property values of T. VM[K,T]=\{ (k,(r,p,v)) \in K\times T/ (r,p,v)\notin S \wedge match(k,v)\le \sigma\}$M[K,T]=MM[K,T]\cup VM[K,T]$: matches between K and T. 答案的顺序：Given a directed graph G, let |G| denote the number of nodes and edges of G and #c(G) denote the number of connected components of G, when the direction of the edges of G is disregarded. We define a partial order “&lt;” for graphs such that, given two graphs G and G’, G]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>2017年8月</tag>
        <tag>RDF</tag>
        <tag>keyword</tag>
        <tag>SPARQL</tag>
      </tags>
  </entry>
</search>
